{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit GnB Training accuracy is: [96.58385093167702] %\n",
      "Scikit GnB Test accuracy is: [64.35643564356435] %\n"
     ]
    }
   ],
   "source": [
    "# Question 1  Naive Bayes Classifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "with open('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 3/fortunecookiedata/traindata.txt', 'r') as f:\n",
    "    Train_x = f.readlines()\n",
    "with open('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 3/fortunecookiedata/trainlabels.txt', 'r') as f:\n",
    "    Train_y = f.readlines()\n",
    "with open('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 3/fortunecookiedata/stoplist.txt', 'r') as f:\n",
    "    Stop = f.readlines() \n",
    "\n",
    "ss = []\n",
    "for i in range(len(Stop)):\n",
    "    ss.append(Stop[i].rstrip()) #Striping the \\n character\n",
    "    \n",
    "voc = []\n",
    "for i in range(len(Train_x)):\n",
    "    x = Train_x[i]\n",
    "    for word in x.split():\n",
    "        if word not in ss:\n",
    "            voc.append(word)        #creating the vocabulary\n",
    "    \n",
    "voc.sort()\n",
    "voc_sort = list(dict.fromkeys(voc))    #alphabetically sorted unique vocabulary\n",
    "\n",
    "\n",
    "# Feature matrix of Training samples\n",
    "X_train = np.zeros((len(Train_x), len(voc_sort)))\n",
    "for i in range(len(Train_x)):\n",
    "    x = Train_x[i]\n",
    "    for j in range(len(voc_sort)):\n",
    "        if x.find(voc_sort[j]) + 1:           \n",
    "            X_train[(i,j)] =  1\n",
    "\n",
    "Y_train = []\n",
    "for i in range(len(Train_y)):\n",
    "    Y_train.append(Train_y[i].rstrip()) #Striping the \\n character\n",
    "      \n",
    "# Feature matrix of Test samples\n",
    "with open('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 3/fortunecookiedata/testdata.txt', 'r') as f:\n",
    "    Test_x = f.readlines()\n",
    "with open('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 3/fortunecookiedata/testlabels.txt', 'r') as f:\n",
    "    Test_y = f.readlines()\n",
    "    \n",
    "        \n",
    "\n",
    "X_test = np.zeros((len(Test_x), len(voc_sort)))\n",
    "for i in range(len(Test_x)):\n",
    "    x = Test_x[i]\n",
    "    for j in range(len(voc_sort)):\n",
    "        if x.find(voc_sort[j]) + 1:           \n",
    "            X_test[(i,j)] =  1   \n",
    "\n",
    "    \n",
    "Y_test = []\n",
    "for i in range(len(Test_y)):\n",
    "    Y_test.append(Test_y[i].rstrip()) #Striping the \\n character\n",
    "    \n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "    \n",
    "##################################################Scikit Function######################################   \n",
    "    \n",
    "gnb = GaussianNB()\n",
    "GnBTraining_acc = []\n",
    "GnBTest_acc = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_train)):\n",
    "    y = gnb.fit(X_train,Y_train).predict([X_train[i,:]])                  # Laplace Smoothing by default\n",
    "    if y != Y_train[i]:\n",
    "        count = count + 1 \n",
    "GnBTraining_acc.append((len(X_train)-count)/len(X_train)*100)\n",
    "\n",
    "    \n",
    "count = 0\n",
    "for i in range(len(X_test)):\n",
    "    y = gnb.fit(X_train,Y_train).predict([X_test[i,:]])                   # Laplace Smoothing by default\n",
    "    if y != Y_test[i]:\n",
    "        count = count + 1 \n",
    "GnBTest_acc.append((len(X_test)-count)/len(X_test)*100)\n",
    "\n",
    "print('Scikit GnB Training accuracy is:', GnBTraining_acc,'%' )\n",
    "print('Scikit GnB Test accuracy is:', GnBTest_acc ,'%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is: [93.7888198757764] %\n",
      "Test accuracy is: [61.386138613861384] %\n"
     ]
    }
   ],
   "source": [
    "#############################################Defining Naive Bayes Classifier###################################\n",
    "\n",
    "c1 = 0\n",
    "for i in range(len(Y_train)):\n",
    "    if int(Y_train[i]) == 1:\n",
    "        c1 = c1 + 1\n",
    "P1 = c1/len(Y_train)\n",
    "\n",
    "Training_acc = []\n",
    "Test_acc = []\n",
    "\n",
    "error = 0\n",
    "for i in range(len(X_train)):\n",
    "    x = X_train[i,:]\n",
    "    mult1 = 1\n",
    "    mult0 = 1\n",
    "    for ii in range(len(voc_sort)):\n",
    "        count = 0\n",
    "        for iii in range(len(Y_train)):\n",
    "            if int(Y_train[iii]) == 1:\n",
    "                if X_train[iii,ii] == x[ii]:\n",
    "                    count = count + 1\n",
    "        # for all y = 1, #count times input feature ii takes that value\n",
    "        Px_i1 = (count/c1)          \n",
    "        mult1 = mult1*Px_i1               # independent events\n",
    "        mult0 = mult0*(1 - Px_i1)\n",
    "            \n",
    "            \n",
    "        \n",
    "    P1 = ((P1*mult1)+1)/(((1-P1)*mult0) + (P1*mult1)+2) #Bayes rule with Laplace smooting\n",
    "    P0 = 1 - P1\n",
    "    y_pred = 0\n",
    "    if P1 > P0:\n",
    "        y_pred = 1\n",
    "    if y_pred != int(Y_train[i]):\n",
    "        error = error + 1\n",
    "        \n",
    "Training_acc.append((len(X_train)-error)/len(X_train)*100)\n",
    "\n",
    "error = 0\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test[i,:]\n",
    "    mult1 = 1\n",
    "    mult0 = 1\n",
    "    for ii in range(len(voc_sort)):\n",
    "        count = 0\n",
    "        for iii in range(len(Y_train)):\n",
    "            if int(Y_train[iii]) == 1:\n",
    "                if X_train[iii,ii] == x[ii]:\n",
    "                    count = count + 1\n",
    "        # for all y = 1, #count times input feature ii takes that value\n",
    "        Px_i1 = (count/c1)          \n",
    "        mult1 = mult1*Px_i1                   # Independent events\n",
    "        mult0 = mult0*(1 - Px_i1)\n",
    "            \n",
    "            \n",
    "        \n",
    "    P1 = ((P1*mult1 + 1))/(((1-P1)*mult0) + (P1*mult1)+2) #Bayes rule with Laplace smoothing\n",
    "    P0 = 1 - P1\n",
    "    y_pred = 0\n",
    "    if P1 > P0:\n",
    "        y_pred = 1\n",
    "    if y_pred != int(Y_test[i]):\n",
    "        error = error + 1\n",
    "        \n",
    "Test_acc.append((len(X_test)-error)/len(X_test)*100)\n",
    "\n",
    "print('Training accuracy is:', Training_acc,'%' )\n",
    "print('Test accuracy is:', Test_acc ,'%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMnistNet(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Q2 CNN on FashionMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# Data label creation\n",
    "def get_data():\n",
    "    train_data = pd.read_csv('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 2/data/fashion-mnist_train.csv')\n",
    "    test_data = pd.read_csv('C:/Users/Chandan/Documents/CPTS 570/Homework/HW 2/data/fashion-mnist_test.csv')\n",
    "    x_train = train_data[train_data.columns[1:]].values\n",
    "    y_train = train_data.label.values\n",
    "    x_test = test_data[test_data.columns[1:]].values\n",
    "    y_test = test_data.label.values\n",
    "    return map(tensor, (x_train, y_train, x_test, y_test)) # maps are useful functions to know\n",
    "                                                           # here, we are just converting lists to pytorch tensors\n",
    "    \n",
    "    \n",
    "x_train, y_train, x_test, y_test = get_data()\n",
    "train_n, train_m = x_train.shape\n",
    "test_n, test_m = x_test.shape\n",
    "n_cls = y_train.max()+1\n",
    "\n",
    "### Normalization\n",
    "x_train, x_test = x_train.float(), x_test.float()\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std\n",
    "def normalize(x, m, s): return (x-m)/s\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test = normalize(x_test, train_mean, train_std) # note this normalize test data also with training mean and standard deviation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definition of the model\n",
    "class FashionMnistNet(nn.Module):\n",
    "    # Based on Lecunn's Lenet architecture\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, stride = 2, padding = 2)   # creating layers\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride = 2, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride = 2, padding = 1)\n",
    "        self.fc1 = nn.Linear(32, 10)\n",
    "        #self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))     #Relu function layer then the max layer (operation)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        pool = nn.AdaptiveAvgPool2d(1)\n",
    "        x = pool(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "# instantiating the model\n",
    "model = FashionMnistNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.2892117500305176\n",
      "loss:  2.307591199874878\n",
      "loss:  2.320528507232666\n",
      "loss:  2.299743890762329\n",
      "loss:  2.3046722412109375\n",
      "loss:  2.298807144165039\n",
      "loss:  2.298743486404419\n",
      "loss:  2.316011428833008\n",
      "loss:  2.2976181507110596\n",
      "loss:  2.3065874576568604\n",
      "loss:  2.3050763607025146\n",
      "loss:  2.3143815994262695\n",
      "loss:  2.273710250854492\n",
      "loss:  2.288867950439453\n",
      "loss:  2.2811737060546875\n",
      "loss:  2.292522430419922\n",
      "loss:  2.316950798034668\n",
      "loss:  2.315214157104492\n",
      "loss:  2.2948451042175293\n",
      "loss:  2.290846347808838\n",
      "loss:  2.2995474338531494\n",
      "loss:  2.304556369781494\n",
      "loss:  2.309980869293213\n",
      "loss:  2.313950300216675\n",
      "loss:  2.298236846923828\n",
      "loss:  2.302234411239624\n",
      "loss:  2.285597324371338\n",
      "loss:  2.3011090755462646\n",
      "loss:  2.297877788543701\n",
      "loss:  2.3077023029327393\n",
      "loss:  2.3087480068206787\n",
      "loss:  2.3158981800079346\n",
      "loss:  2.3074653148651123\n",
      "loss:  2.293834686279297\n",
      "loss:  2.2899649143218994\n",
      "loss:  2.3007595539093018\n",
      "loss:  2.3180384635925293\n",
      "loss:  2.3262977600097656\n",
      "loss:  2.29496169090271\n",
      "loss:  2.3105289936065674\n",
      "loss:  2.2945830821990967\n",
      "loss:  2.316096782684326\n",
      "loss:  2.305642604827881\n",
      "loss:  2.3093602657318115\n",
      "loss:  2.2676215171813965\n",
      "loss:  2.287579298019409\n",
      "loss:  2.308788299560547\n",
      "loss:  2.318849563598633\n",
      "loss:  2.2994792461395264\n",
      "loss:  2.2990007400512695\n",
      "loss:  2.3078601360321045\n",
      "loss:  2.300816297531128\n",
      "loss:  2.301532030105591\n",
      "loss:  2.3094427585601807\n",
      "loss:  2.3147401809692383\n",
      "loss:  2.312582015991211\n",
      "loss:  2.2845516204833984\n",
      "loss:  2.3093857765197754\n",
      "loss:  2.3000292778015137\n",
      "loss:  2.30060076713562\n",
      "loss:  2.3009026050567627\n",
      "loss:  2.29919695854187\n",
      "loss:  2.3092401027679443\n",
      "loss:  2.306023597717285\n",
      "loss:  2.3064887523651123\n",
      "loss:  2.3240785598754883\n",
      "loss:  2.3058581352233887\n",
      "loss:  2.300504684448242\n",
      "loss:  2.2877397537231445\n",
      "loss:  2.2997841835021973\n",
      "loss:  2.3153631687164307\n",
      "loss:  2.309234380722046\n",
      "loss:  2.3123106956481934\n",
      "loss:  2.3009469509124756\n",
      "loss:  2.302941083908081\n",
      "loss:  2.319005012512207\n",
      "loss:  2.3169798851013184\n",
      "loss:  2.319692611694336\n",
      "loss:  2.2902581691741943\n",
      "loss:  2.29609751701355\n",
      "loss:  2.30491304397583\n",
      "loss:  2.296776056289673\n",
      "loss:  2.3126296997070312\n",
      "loss:  2.300945281982422\n",
      "loss:  2.302962303161621\n",
      "loss:  2.3186659812927246\n",
      "loss:  2.2924468517303467\n",
      "loss:  2.3113081455230713\n",
      "loss:  2.313706636428833\n",
      "loss:  2.296955108642578\n",
      "loss:  2.3092517852783203\n",
      "loss:  2.2846779823303223\n",
      "loss:  2.296842575073242\n",
      "loss:  2.3187925815582275\n",
      "loss:  2.295346736907959\n",
      "loss:  2.3148112297058105\n",
      "loss:  2.300088405609131\n",
      "loss:  2.3009119033813477\n",
      "loss:  2.3193118572235107\n",
      "loss:  2.3106868267059326\n",
      "loss:  2.299264907836914\n",
      "loss:  2.3205385208129883\n",
      "loss:  2.2749404907226562\n",
      "loss:  2.2908360958099365\n",
      "loss:  2.324838876724243\n",
      "loss:  2.3174753189086914\n",
      "loss:  2.3033580780029297\n",
      "loss:  2.3102807998657227\n",
      "loss:  2.303250551223755\n",
      "loss:  2.3144896030426025\n",
      "loss:  2.3020708560943604\n",
      "loss:  2.2991061210632324\n",
      "loss:  2.3133795261383057\n",
      "loss:  2.290879011154175\n",
      "loss:  2.310542583465576\n",
      "loss:  2.3123834133148193\n",
      "loss:  2.2985482215881348\n",
      "loss:  2.32600998878479\n",
      "loss:  2.296470880508423\n",
      "loss:  2.280271053314209\n",
      "loss:  2.2984657287597656\n",
      "loss:  2.2926738262176514\n",
      "loss:  2.287489891052246\n",
      "loss:  2.320009469985962\n",
      "loss:  2.303558111190796\n",
      "loss:  2.3327255249023438\n",
      "loss:  2.2980053424835205\n",
      "loss:  2.305234670639038\n",
      "loss:  2.3025500774383545\n",
      "loss:  2.3143622875213623\n",
      "loss:  2.3121907711029053\n",
      "loss:  2.308659315109253\n",
      "loss:  2.3109567165374756\n",
      "loss:  2.304868221282959\n",
      "loss:  2.3157808780670166\n",
      "loss:  2.3035154342651367\n",
      "loss:  2.307807683944702\n",
      "loss:  2.3083152770996094\n",
      "loss:  2.308837890625\n",
      "loss:  2.3076252937316895\n",
      "loss:  2.31225323677063\n",
      "loss:  2.3188581466674805\n",
      "loss:  2.292809009552002\n",
      "loss:  2.301262855529785\n",
      "loss:  2.307647466659546\n",
      "loss:  2.290351152420044\n",
      "loss:  2.298119306564331\n",
      "loss:  2.3038175106048584\n",
      "loss:  2.2996644973754883\n",
      "loss:  2.3191826343536377\n",
      "loss:  2.295273780822754\n",
      "loss:  2.3007500171661377\n",
      "loss:  2.3169450759887695\n",
      "loss:  2.2905590534210205\n",
      "loss:  2.3244612216949463\n",
      "loss:  2.2897531986236572\n",
      "loss:  2.3104965686798096\n",
      "loss:  2.295773983001709\n",
      "loss:  2.3037619590759277\n",
      "loss:  2.294438362121582\n",
      "loss:  2.2929701805114746\n",
      "loss:  2.284564256668091\n",
      "loss:  2.2926621437072754\n",
      "loss:  2.290311813354492\n",
      "loss:  2.292224645614624\n",
      "loss:  2.2917728424072266\n",
      "loss:  2.3088324069976807\n",
      "loss:  2.3074076175689697\n",
      "loss:  2.3039355278015137\n",
      "loss:  2.2855231761932373\n",
      "loss:  2.3331031799316406\n",
      "loss:  2.296842336654663\n",
      "loss:  2.298365592956543\n",
      "loss:  2.2989704608917236\n",
      "loss:  2.295260429382324\n",
      "loss:  2.3053619861602783\n",
      "loss:  2.306457281112671\n",
      "loss:  2.3137471675872803\n",
      "loss:  2.310197114944458\n",
      "loss:  2.2895588874816895\n",
      "loss:  2.3175737857818604\n",
      "loss:  2.32363224029541\n",
      "loss:  2.307610034942627\n",
      "loss:  2.30288028717041\n",
      "loss:  2.297985792160034\n",
      "loss:  2.2963051795959473\n",
      "loss:  2.303776264190674\n",
      "loss:  2.3228209018707275\n",
      "loss:  2.3240275382995605\n",
      "loss:  2.2986044883728027\n",
      "loss:  2.2970175743103027\n",
      "loss:  2.310636520385742\n",
      "loss:  2.3277225494384766\n",
      "loss:  2.2935073375701904\n",
      "loss:  2.295316696166992\n",
      "loss:  2.299240827560425\n",
      "loss:  2.2972867488861084\n",
      "loss:  2.3106753826141357\n",
      "loss:  2.3146042823791504\n",
      "loss:  2.2916905879974365\n",
      "loss:  2.309999465942383\n",
      "loss:  2.3132896423339844\n",
      "loss:  2.311595916748047\n",
      "loss:  2.32045841217041\n",
      "loss:  2.318506956100464\n",
      "loss:  2.2973742485046387\n",
      "loss:  2.301010847091675\n",
      "loss:  2.3009700775146484\n",
      "loss:  2.307563543319702\n",
      "loss:  2.303865909576416\n",
      "loss:  2.320145845413208\n",
      "loss:  2.3049681186676025\n",
      "loss:  2.29815936088562\n",
      "loss:  2.3265063762664795\n",
      "loss:  2.3037068843841553\n",
      "loss:  2.3010265827178955\n",
      "loss:  2.3004140853881836\n",
      "loss:  2.2987167835235596\n",
      "loss:  2.2996296882629395\n",
      "loss:  2.302680253982544\n",
      "loss:  2.3018226623535156\n",
      "loss:  2.3068270683288574\n",
      "loss:  2.298285484313965\n",
      "loss:  2.2961575984954834\n",
      "loss:  2.3112728595733643\n",
      "loss:  2.3067901134490967\n",
      "loss:  2.304948091506958\n",
      "loss:  2.306504249572754\n",
      "loss:  2.292949676513672\n",
      "loss:  2.3125858306884766\n",
      "loss:  2.307020664215088\n",
      "loss:  2.3071343898773193\n",
      "loss:  2.282153606414795\n",
      "loss:  2.3219144344329834\n",
      "loss:  2.2906041145324707\n",
      "loss:  2.300577163696289\n",
      "loss:  2.300924062728882\n",
      "loss:  2.324653387069702\n",
      "loss:  2.3144619464874268\n",
      "loss:  2.2995762825012207\n",
      "loss:  2.2939369678497314\n",
      "loss:  2.3099794387817383\n",
      "loss:  2.298952102661133\n",
      "loss:  2.3093159198760986\n",
      "loss:  2.3016977310180664\n",
      "loss:  2.3077096939086914\n",
      "loss:  2.309495449066162\n",
      "loss:  2.3194217681884766\n",
      "loss:  2.2953193187713623\n",
      "loss:  2.320671796798706\n",
      "loss:  2.323704242706299\n",
      "loss:  2.313756227493286\n",
      "loss:  2.2922909259796143\n",
      "loss:  2.3093700408935547\n",
      "loss:  2.2925117015838623\n",
      "loss:  2.2956042289733887\n",
      "loss:  2.3042569160461426\n",
      "loss:  2.314239025115967\n",
      "loss:  2.3228015899658203\n",
      "loss:  2.316066026687622\n",
      "loss:  2.302248477935791\n",
      "loss:  2.304574489593506\n",
      "loss:  2.3057498931884766\n",
      "loss:  2.2899680137634277\n",
      "loss:  2.282705783843994\n",
      "loss:  2.2867445945739746\n",
      "loss:  2.301748514175415\n",
      "loss:  2.3168015480041504\n",
      "loss:  2.2950093746185303\n",
      "loss:  2.294325113296509\n",
      "loss:  2.2947046756744385\n",
      "loss:  2.318009853363037\n",
      "loss:  2.325953960418701\n",
      "loss:  2.304208278656006\n",
      "loss:  2.2968194484710693\n",
      "loss:  2.2934112548828125\n",
      "loss:  2.305413246154785\n",
      "loss:  2.3001348972320557\n",
      "loss:  2.286306142807007\n",
      "loss:  2.3177928924560547\n",
      "loss:  2.2933125495910645\n",
      "loss:  2.320371627807617\n",
      "loss:  2.2929604053497314\n",
      "loss:  2.3109047412872314\n",
      "loss:  2.2872893810272217\n",
      "loss:  2.3112452030181885\n",
      "loss:  2.2930338382720947\n",
      "loss:  2.306506395339966\n",
      "loss:  2.2968642711639404\n",
      "loss:  2.309429168701172\n",
      "loss:  2.3034558296203613\n",
      "loss:  2.303577423095703\n",
      "loss:  2.3043429851531982\n",
      "loss:  2.3029379844665527\n",
      "loss:  2.2867202758789062\n",
      "loss:  2.300804853439331\n",
      "loss:  2.3094944953918457\n",
      "loss:  2.2994933128356934\n",
      "loss:  2.3166542053222656\n",
      "loss:  2.315514326095581\n",
      "loss:  2.320793628692627\n",
      "loss:  2.2941389083862305\n",
      "loss:  2.3018853664398193\n",
      "loss:  2.325075387954712\n",
      "loss:  2.3028738498687744\n",
      "loss:  2.308974504470825\n",
      "loss:  2.290782928466797\n",
      "loss:  2.297640562057495\n",
      "loss:  2.3081436157226562\n",
      "loss:  2.3102173805236816\n",
      "loss:  2.3072433471679688\n",
      "loss:  2.3124682903289795\n",
      "loss:  2.325098991394043\n",
      "loss:  2.304455041885376\n",
      "loss:  2.304920196533203\n",
      "loss:  2.311513662338257\n",
      "loss:  2.2999563217163086\n",
      "loss:  2.300642967224121\n",
      "loss:  2.3065590858459473\n",
      "loss:  2.294048309326172\n",
      "loss:  2.308389663696289\n",
      "loss:  2.3226425647735596\n",
      "loss:  2.289430618286133\n",
      "loss:  2.2894504070281982\n",
      "loss:  2.313931941986084\n",
      "loss:  2.30049467086792\n",
      "loss:  2.3199052810668945\n",
      "loss:  2.311966896057129\n",
      "loss:  2.3193655014038086\n",
      "loss:  2.2891905307769775\n",
      "loss:  2.298060417175293\n",
      "loss:  2.3010616302490234\n",
      "loss:  2.2842798233032227\n",
      "loss:  2.3166308403015137\n",
      "loss:  2.3064534664154053\n",
      "loss:  2.3101749420166016\n",
      "loss:  2.304548740386963\n",
      "loss:  2.300800323486328\n",
      "loss:  2.295201301574707\n",
      "loss:  2.3125698566436768\n",
      "loss:  2.3031396865844727\n",
      "loss:  2.3183255195617676\n",
      "loss:  2.305865526199341\n",
      "loss:  2.3047378063201904\n",
      "loss:  2.3129172325134277\n",
      "loss:  2.302379846572876\n",
      "loss:  2.3114867210388184\n",
      "loss:  2.2956159114837646\n",
      "loss:  2.300297737121582\n",
      "loss:  2.311309814453125\n",
      "loss:  2.311917543411255\n",
      "loss:  2.282123327255249\n",
      "loss:  2.312237024307251\n",
      "loss:  2.3081507682800293\n",
      "loss:  2.308291435241699\n",
      "loss:  2.276837110519409\n",
      "loss:  2.300278663635254\n",
      "loss:  2.313941717147827\n",
      "loss:  2.3121590614318848\n",
      "loss:  2.3057844638824463\n",
      "loss:  2.277313232421875\n",
      "loss:  2.299656629562378\n",
      "loss:  2.3042445182800293\n",
      "loss:  2.302269458770752\n",
      "loss:  2.313485622406006\n",
      "loss:  2.3195767402648926\n",
      "loss:  2.2984509468078613\n",
      "loss:  2.3070790767669678\n",
      "loss:  2.3115458488464355\n",
      "loss:  2.285351276397705\n",
      "loss:  2.3007678985595703\n",
      "loss:  2.318413019180298\n",
      "loss:  2.307745933532715\n",
      "loss:  2.325916051864624\n",
      "loss:  2.3106093406677246\n",
      "loss:  2.319586753845215\n",
      "loss:  2.3082633018493652\n",
      "loss:  2.2959060668945312\n",
      "loss:  2.2960948944091797\n",
      "loss:  2.3009047508239746\n",
      "loss:  2.308652877807617\n",
      "loss:  2.307612180709839\n",
      "loss:  2.298187255859375\n",
      "loss:  2.2947633266448975\n",
      "loss:  2.3003194332122803\n",
      "loss:  2.308476448059082\n",
      "loss:  2.2825052738189697\n",
      "loss:  2.2992000579833984\n",
      "loss:  2.2953858375549316\n",
      "loss:  2.319119453430176\n",
      "loss:  2.3003015518188477\n",
      "loss:  2.296304941177368\n",
      "loss:  2.283353805541992\n",
      "loss:  2.2934985160827637\n",
      "loss:  2.290579319000244\n",
      "loss:  2.300351858139038\n",
      "loss:  2.292983293533325\n",
      "loss:  2.28664231300354\n",
      "loss:  2.3092567920684814\n",
      "loss:  2.320580244064331\n",
      "loss:  2.298574209213257\n",
      "loss:  2.308370590209961\n",
      "loss:  2.3010456562042236\n",
      "loss:  2.311123847961426\n",
      "loss:  2.2963461875915527\n",
      "loss:  2.290048599243164\n",
      "loss:  2.307806968688965\n",
      "loss:  2.306962251663208\n",
      "loss:  2.2983713150024414\n",
      "loss:  2.298457145690918\n",
      "loss:  2.306859254837036\n",
      "loss:  2.3008344173431396\n",
      "loss:  2.291893720626831\n",
      "loss:  2.299067974090576\n",
      "loss:  2.318235397338867\n",
      "loss:  2.309201955795288\n",
      "loss:  2.3198368549346924\n",
      "loss:  2.306093692779541\n",
      "loss:  2.3082664012908936\n",
      "loss:  2.3110358715057373\n",
      "loss:  2.3211865425109863\n",
      "loss:  2.3088014125823975\n",
      "loss:  2.2917542457580566\n",
      "loss:  2.3057687282562256\n",
      "loss:  2.3103864192962646\n",
      "loss:  2.309142827987671\n",
      "loss:  2.2938544750213623\n",
      "loss:  2.300830841064453\n",
      "loss:  2.2968497276306152\n",
      "loss:  2.3078322410583496\n",
      "loss:  2.3128604888916016\n",
      "loss:  2.306410312652588\n",
      "loss:  2.3285114765167236\n",
      "loss:  2.312249183654785\n",
      "loss:  2.291025400161743\n",
      "loss:  2.2822680473327637\n",
      "loss:  2.3048624992370605\n",
      "loss:  2.3220009803771973\n",
      "loss:  2.305473566055298\n",
      "loss:  2.306450843811035\n",
      "loss:  2.3005259037017822\n",
      "loss:  2.2878668308258057\n",
      "loss:  2.301539182662964\n",
      "loss:  2.3180558681488037\n",
      "loss:  2.2976489067077637\n",
      "loss:  2.3259756565093994\n",
      "loss:  2.317734718322754\n",
      "loss:  2.2993152141571045\n",
      "loss:  2.3062264919281006\n",
      "loss:  2.2987277507781982\n",
      "loss:  2.325632095336914\n",
      "loss:  2.298367738723755\n",
      "loss:  2.3083431720733643\n",
      "loss:  2.290609836578369\n",
      "loss:  2.3145720958709717\n",
      "loss:  2.3199622631073\n",
      "loss:  2.3066909313201904\n",
      "loss:  2.2987220287323\n",
      "loss:  2.303226947784424\n",
      "loss:  2.3086206912994385\n",
      "loss:  2.3208088874816895\n",
      "loss:  2.2984120845794678\n",
      "loss:  2.3198137283325195\n",
      "loss:  2.3043408393859863\n",
      "loss:  2.3018393516540527\n",
      "loss:  2.2910215854644775\n",
      "loss:  2.2888519763946533\n",
      "loss:  2.2938461303710938\n",
      "loss:  2.315239667892456\n",
      "loss:  2.293922185897827\n",
      "loss:  2.3096156120300293\n",
      "loss:  2.279606819152832\n",
      "loss:  2.314664840698242\n",
      "loss:  2.2982170581817627\n",
      "loss:  2.294445514678955\n",
      "loss:  2.299406051635742\n",
      "loss:  2.2995588779449463\n",
      "loss:  2.296417474746704\n",
      "loss:  2.3057079315185547\n",
      "loss:  2.306335210800171\n",
      "loss:  2.311624765396118\n",
      "loss:  2.3107388019561768\n",
      "loss:  2.2926228046417236\n",
      "loss:  2.29824161529541\n",
      "loss:  2.307088851928711\n",
      "loss:  2.289964437484741\n",
      "loss:  2.301403284072876\n",
      "loss:  2.3126442432403564\n",
      "loss:  2.3081233501434326\n",
      "loss:  2.3199410438537598\n",
      "loss:  2.2838261127471924\n",
      "loss:  2.2888522148132324\n",
      "loss:  2.308701992034912\n",
      "loss:  2.3021445274353027\n",
      "loss:  2.30141544342041\n",
      "loss:  2.2805984020233154\n",
      "loss:  2.2903809547424316\n",
      "loss:  2.306492805480957\n",
      "loss:  2.284928321838379\n",
      "loss:  2.308779001235962\n",
      "loss:  2.30846905708313\n",
      "loss:  2.2964301109313965\n",
      "loss:  2.3078269958496094\n",
      "loss:  2.319176197052002\n",
      "loss:  2.304363489151001\n",
      "loss:  2.3150861263275146\n",
      "loss:  2.3145978450775146\n",
      "loss:  2.3032476902008057\n",
      "loss:  2.302717685699463\n",
      "loss:  2.302603006362915\n",
      "loss:  2.312391757965088\n",
      "loss:  2.289874792098999\n",
      "loss:  2.3172731399536133\n",
      "loss:  2.2951698303222656\n",
      "loss:  2.308358669281006\n",
      "loss:  2.3034894466400146\n",
      "loss:  2.3193740844726562\n",
      "loss:  2.311427116394043\n",
      "loss:  2.2936174869537354\n",
      "loss:  2.3217053413391113\n",
      "loss:  2.3123371601104736\n",
      "loss:  2.2983627319335938\n",
      "loss:  2.290189504623413\n",
      "loss:  2.3158321380615234\n",
      "loss:  2.311171054840088\n",
      "loss:  2.3012516498565674\n",
      "loss:  2.2999460697174072\n",
      "loss:  2.312272787094116\n",
      "loss:  2.288886785507202\n",
      "loss:  2.3111422061920166\n",
      "loss:  2.2965211868286133\n",
      "loss:  2.2985618114471436\n",
      "loss:  2.300954818725586\n",
      "loss:  2.302049398422241\n",
      "loss:  2.315800428390503\n",
      "loss:  2.30692195892334\n",
      "loss:  2.305277109146118\n",
      "loss:  2.300929307937622\n",
      "loss:  2.3199872970581055\n",
      "loss:  2.2806801795959473\n",
      "loss:  2.3155384063720703\n",
      "loss:  2.316457748413086\n",
      "loss:  2.3044893741607666\n",
      "loss:  2.3133010864257812\n",
      "loss:  2.2988569736480713\n",
      "loss:  2.3110010623931885\n",
      "loss:  2.328122615814209\n",
      "loss:  2.3053245544433594\n",
      "loss:  2.305236577987671\n",
      "loss:  2.3245813846588135\n",
      "loss:  2.322786808013916\n",
      "loss:  2.306736946105957\n",
      "loss:  2.3166675567626953\n",
      "loss:  2.2927286624908447\n",
      "loss:  2.29770565032959\n",
      "loss:  2.302053451538086\n",
      "loss:  2.3080806732177734\n",
      "loss:  2.3243815898895264\n",
      "loss:  2.2927825450897217\n",
      "loss:  2.3065316677093506\n",
      "loss:  2.284409761428833\n",
      "loss:  2.298987865447998\n",
      "loss:  2.294764995574951\n",
      "loss:  2.293548345565796\n",
      "loss:  2.2982118129730225\n",
      "loss:  2.288933038711548\n",
      "loss:  2.3010761737823486\n",
      "loss:  2.314100742340088\n",
      "loss:  2.2973685264587402\n",
      "loss:  2.3179194927215576\n",
      "loss:  2.300673484802246\n",
      "loss:  2.3072822093963623\n",
      "loss:  2.3022048473358154\n",
      "loss:  2.3240346908569336\n",
      "loss:  2.312716484069824\n",
      "loss:  2.2969932556152344\n",
      "loss:  2.285921335220337\n",
      "loss:  2.3019282817840576\n",
      "loss:  2.3139729499816895\n",
      "loss:  2.3180091381073\n",
      "loss:  2.314880132675171\n",
      "loss:  2.296557664871216\n",
      "loss:  2.2987654209136963\n",
      "loss:  2.2954745292663574\n",
      "loss:  2.3037970066070557\n",
      "loss:  2.300525188446045\n",
      "loss:  2.3105764389038086\n",
      "loss:  2.303983211517334\n",
      "loss:  2.3008012771606445\n",
      "loss:  2.3024206161499023\n",
      "loss:  2.3058910369873047\n",
      "loss:  2.2949442863464355\n",
      "loss:  2.311082363128662\n",
      "loss:  2.290712594985962\n",
      "loss:  2.302126169204712\n",
      "loss:  2.3028814792633057\n",
      "loss:  2.2951056957244873\n",
      "loss:  2.307499408721924\n",
      "loss:  2.2880959510803223\n",
      "loss:  2.296215534210205\n",
      "loss:  2.3087315559387207\n",
      "loss:  2.317365884780884\n",
      "loss:  2.296190023422241\n",
      "loss:  2.2778584957122803\n",
      "loss:  2.316002130508423\n",
      "loss:  2.3057520389556885\n",
      "loss:  2.3110713958740234\n",
      "loss:  2.308427095413208\n",
      "loss:  2.3103220462799072\n",
      "loss:  2.2889654636383057\n",
      "loss:  2.3015451431274414\n",
      "loss:  2.303652763366699\n",
      "loss:  2.322788715362549\n",
      "loss:  2.3004255294799805\n",
      "loss:  2.2986485958099365\n",
      "loss:  2.3165266513824463\n",
      "loss:  2.331164598464966\n",
      "loss:  2.2979564666748047\n",
      "loss:  2.2874112129211426\n",
      "loss:  2.306804895401001\n",
      "loss:  2.2941651344299316\n",
      "loss:  2.3175055980682373\n",
      "loss:  2.296175956726074\n",
      "loss:  2.285350799560547\n",
      "loss:  2.315967559814453\n",
      "loss:  2.3017830848693848\n",
      "loss:  2.305864095687866\n",
      "loss:  2.2975926399230957\n",
      "loss:  2.3152060508728027\n",
      "loss:  2.319491386413574\n",
      "loss:  2.326718330383301\n",
      "loss:  2.2988696098327637\n",
      "loss:  2.321054220199585\n",
      "loss:  2.3085899353027344\n",
      "loss:  2.2945141792297363\n",
      "loss:  2.3135318756103516\n",
      "loss:  2.293255090713501\n",
      "loss:  2.323441743850708\n",
      "loss:  2.2935779094696045\n",
      "loss:  2.3050708770751953\n",
      "loss:  2.293975353240967\n",
      "loss:  2.314756155014038\n",
      "loss:  2.3242461681365967\n",
      "loss:  2.304309368133545\n",
      "loss:  2.285895824432373\n",
      "loss:  2.296710252761841\n",
      "loss:  2.3059844970703125\n",
      "loss:  2.308335781097412\n",
      "loss:  2.2991461753845215\n",
      "loss:  2.3266632556915283\n",
      "loss:  2.305652618408203\n",
      "loss:  2.3086252212524414\n",
      "loss:  2.3064987659454346\n",
      "loss:  2.3317959308624268\n",
      "loss:  2.3077480792999268\n",
      "loss:  2.2791635990142822\n",
      "loss:  2.3115267753601074\n",
      "loss:  2.3171160221099854\n",
      "loss:  2.3215808868408203\n",
      "loss:  2.2953097820281982\n",
      "loss:  2.312300205230713\n",
      "loss:  2.3042826652526855\n",
      "loss:  2.305422067642212\n",
      "loss:  2.3158226013183594\n",
      "loss:  2.295030117034912\n",
      "loss:  2.3051815032958984\n",
      "loss:  2.2935402393341064\n",
      "loss:  2.2918319702148438\n",
      "loss:  2.3161003589630127\n",
      "loss:  2.308884382247925\n",
      "loss:  2.2965235710144043\n",
      "loss:  2.3063714504241943\n",
      "loss:  2.3121767044067383\n",
      "loss:  2.297344923019409\n",
      "loss:  2.3084731101989746\n",
      "loss:  2.298849105834961\n",
      "loss:  2.303513765335083\n",
      "loss:  2.306490182876587\n",
      "loss:  2.324693441390991\n",
      "loss:  2.3212249279022217\n",
      "loss:  2.3130557537078857\n",
      "loss:  2.304459571838379\n",
      "loss:  2.2966017723083496\n",
      "loss:  2.306913137435913\n",
      "loss:  2.3059043884277344\n",
      "loss:  2.290297031402588\n",
      "loss:  2.323967933654785\n",
      "loss:  2.316033124923706\n",
      "loss:  2.297752857208252\n",
      "loss:  2.3080618381500244\n",
      "loss:  2.32216477394104\n",
      "loss:  2.2912638187408447\n",
      "loss:  2.318697690963745\n",
      "loss:  2.292538642883301\n",
      "loss:  2.303584575653076\n",
      "loss:  2.3096022605895996\n",
      "loss:  2.3027420043945312\n",
      "loss:  2.320115327835083\n",
      "loss:  2.3247063159942627\n",
      "loss:  2.299920082092285\n",
      "loss:  2.3013641834259033\n",
      "loss:  2.3152098655700684\n",
      "loss:  2.281578302383423\n",
      "loss:  2.3016304969787598\n",
      "loss:  2.311899185180664\n",
      "loss:  2.315631151199341\n",
      "loss:  2.2980844974517822\n",
      "loss:  2.3147974014282227\n",
      "loss:  2.298150062561035\n",
      "loss:  2.3106939792633057\n",
      "loss:  2.312973976135254\n",
      "loss:  2.2949557304382324\n",
      "loss:  2.323265790939331\n",
      "loss:  2.3228659629821777\n",
      "loss:  2.306048631668091\n",
      "loss:  2.2988545894622803\n",
      "loss:  2.289813280105591\n",
      "loss:  2.2976436614990234\n",
      "loss:  2.308422803878784\n",
      "loss:  2.3098268508911133\n",
      "loss:  2.2995762825012207\n",
      "loss:  2.30377459526062\n",
      "loss:  2.3113162517547607\n",
      "loss:  2.3024380207061768\n",
      "loss:  2.3190407752990723\n",
      "loss:  2.3183982372283936\n",
      "loss:  2.3090271949768066\n",
      "loss:  2.300060749053955\n",
      "loss:  2.298327684402466\n",
      "loss:  2.291351556777954\n",
      "loss:  2.297539710998535\n",
      "loss:  2.3035428524017334\n",
      "loss:  2.310105085372925\n",
      "loss:  2.3073577880859375\n",
      "loss:  2.2988944053649902\n",
      "loss:  2.2890241146087646\n",
      "loss:  2.3017778396606445\n",
      "loss:  2.3119077682495117\n",
      "loss:  2.319152355194092\n",
      "loss:  2.281705617904663\n",
      "loss:  2.2836122512817383\n",
      "loss:  2.3155715465545654\n",
      "loss:  2.299109935760498\n",
      "loss:  2.297147750854492\n",
      "loss:  2.2946410179138184\n",
      "loss:  2.3058853149414062\n",
      "loss:  2.314821720123291\n",
      "loss:  2.30910062789917\n",
      "loss:  2.3099567890167236\n",
      "loss:  2.307527780532837\n",
      "loss:  2.3226535320281982\n",
      "loss:  2.3127527236938477\n",
      "loss:  2.3294432163238525\n",
      "loss:  2.3057923316955566\n",
      "loss:  2.312147378921509\n",
      "loss:  2.3080782890319824\n",
      "loss:  2.3222174644470215\n",
      "loss:  2.2861368656158447\n",
      "loss:  2.3127410411834717\n",
      "loss:  2.305896759033203\n",
      "loss:  2.309900999069214\n",
      "loss:  2.2981150150299072\n",
      "loss:  2.3093643188476562\n",
      "loss:  2.2928638458251953\n",
      "loss:  2.307861804962158\n",
      "loss:  2.322683095932007\n",
      "loss:  2.298492670059204\n",
      "loss:  2.2981128692626953\n",
      "loss:  2.305873394012451\n",
      "loss:  2.31961989402771\n",
      "loss:  2.3045005798339844\n",
      "loss:  2.3000268936157227\n",
      "loss:  2.2933685779571533\n",
      "loss:  2.3090808391571045\n",
      "loss:  2.29115629196167\n",
      "loss:  2.313009262084961\n",
      "loss:  2.3149826526641846\n",
      "loss:  2.3057169914245605\n",
      "loss:  2.291525363922119\n",
      "loss:  2.3065452575683594\n",
      "loss:  2.289818286895752\n",
      "loss:  2.3168160915374756\n",
      "loss:  2.3155980110168457\n",
      "loss:  2.3131039142608643\n",
      "loss:  2.3165676593780518\n",
      "loss:  2.317387104034424\n",
      "loss:  2.2946207523345947\n",
      "loss:  2.3141660690307617\n",
      "loss:  2.299978256225586\n",
      "loss:  2.310673236846924\n",
      "loss:  2.300689697265625\n",
      "loss:  2.3058865070343018\n",
      "loss:  2.3092777729034424\n",
      "loss:  2.293253183364868\n",
      "loss:  2.3005213737487793\n",
      "loss:  2.3224334716796875\n",
      "loss:  2.2860279083251953\n",
      "loss:  2.312479019165039\n",
      "loss:  2.3216006755828857\n",
      "loss:  2.3049652576446533\n",
      "loss:  2.316596269607544\n",
      "loss:  2.2883431911468506\n",
      "loss:  2.304229736328125\n",
      "loss:  2.305394172668457\n",
      "loss:  2.3108954429626465\n",
      "loss:  2.298011302947998\n",
      "loss:  2.302215099334717\n",
      "loss:  2.300790309906006\n",
      "loss:  2.3035926818847656\n",
      "loss:  2.2886946201324463\n",
      "loss:  2.31679630279541\n",
      "loss:  2.2895219326019287\n",
      "loss:  2.315330743789673\n",
      "loss:  2.318664789199829\n",
      "loss:  2.3022570610046387\n",
      "loss:  2.318455696105957\n",
      "loss:  2.31942081451416\n",
      "loss:  2.30767560005188\n",
      "loss:  2.2904112339019775\n",
      "loss:  2.3038594722747803\n",
      "loss:  2.3227293491363525\n",
      "loss:  2.320420742034912\n",
      "loss:  2.3008296489715576\n",
      "loss:  2.3123273849487305\n",
      "loss:  2.313727617263794\n",
      "loss:  2.3150956630706787\n",
      "loss:  2.307602643966675\n",
      "loss:  2.3163833618164062\n",
      "loss:  2.29172682762146\n",
      "loss:  2.2971127033233643\n",
      "loss:  2.2983036041259766\n",
      "loss:  2.3005850315093994\n",
      "loss:  2.30694317817688\n",
      "loss:  2.2913835048675537\n",
      "loss:  2.3005728721618652\n",
      "loss:  2.3113620281219482\n",
      "loss:  2.307581663131714\n",
      "loss:  2.3159663677215576\n",
      "loss:  2.3047327995300293\n",
      "loss:  2.317230701446533\n",
      "loss:  2.3005716800689697\n",
      "loss:  2.2805628776550293\n",
      "loss:  2.3010518550872803\n",
      "loss:  2.2961809635162354\n",
      "loss:  2.284043073654175\n",
      "loss:  2.30361008644104\n",
      "loss:  2.3095273971557617\n",
      "loss:  2.3181955814361572\n",
      "loss:  2.3105838298797607\n",
      "loss:  2.319566011428833\n",
      "loss:  2.301588535308838\n",
      "loss:  2.281838893890381\n",
      "loss:  2.3058481216430664\n",
      "loss:  2.308332681655884\n",
      "loss:  2.2997548580169678\n",
      "loss:  2.3060054779052734\n",
      "loss:  2.3072943687438965\n",
      "loss:  2.3197073936462402\n",
      "loss:  2.305055856704712\n",
      "loss:  2.3057310581207275\n",
      "loss:  2.31709885597229\n",
      "loss:  2.305255889892578\n",
      "loss:  2.3259084224700928\n",
      "loss:  2.3162662982940674\n",
      "loss:  2.2920684814453125\n",
      "loss:  2.3154797554016113\n",
      "loss:  2.3209402561187744\n",
      "loss:  2.295607089996338\n",
      "loss:  2.298928737640381\n",
      "loss:  2.3165652751922607\n",
      "loss:  2.3067831993103027\n",
      "loss:  2.313131332397461\n",
      "loss:  2.295780897140503\n",
      "loss:  2.302672863006592\n",
      "loss:  2.3365235328674316\n",
      "loss:  2.3073041439056396\n",
      "loss:  2.2872655391693115\n",
      "loss:  2.300600051879883\n",
      "loss:  2.307316541671753\n",
      "loss:  2.288335084915161\n",
      "loss:  2.29408597946167\n",
      "loss:  2.3025145530700684\n",
      "loss:  2.30985164642334\n",
      "loss:  2.323484182357788\n",
      "loss:  2.314234495162964\n",
      "loss:  2.2922003269195557\n",
      "loss:  2.2941653728485107\n",
      "loss:  2.3042705059051514\n",
      "loss:  2.3070781230926514\n",
      "loss:  2.296830654144287\n",
      "loss:  2.283658266067505\n",
      "loss:  2.302868366241455\n",
      "loss:  2.2887377738952637\n",
      "loss:  2.311769723892212\n",
      "loss:  2.311298370361328\n",
      "loss:  2.302177667617798\n",
      "loss:  2.3169472217559814\n",
      "loss:  2.293509006500244\n",
      "loss:  2.299680709838867\n",
      "loss:  2.3071558475494385\n",
      "loss:  2.307443618774414\n",
      "loss:  2.3026490211486816\n",
      "loss:  2.2886950969696045\n",
      "loss:  2.3046936988830566\n",
      "loss:  2.303987979888916\n",
      "loss:  2.2983710765838623\n",
      "loss:  2.3081471920013428\n",
      "loss:  2.3051860332489014\n",
      "loss:  2.3191537857055664\n",
      "loss:  2.3136560916900635\n",
      "loss:  2.303215980529785\n",
      "loss:  2.297379732131958\n",
      "loss:  2.2882332801818848\n",
      "loss:  2.282257556915283\n",
      "loss:  2.295067310333252\n",
      "loss:  2.3242342472076416\n",
      "loss:  2.327049732208252\n",
      "loss:  2.2974014282226562\n",
      "loss:  2.2839105129241943\n",
      "loss:  2.306976079940796\n",
      "loss:  2.2900612354278564\n",
      "loss:  2.2900149822235107\n",
      "loss:  2.305331230163574\n",
      "loss:  2.2867484092712402\n",
      "loss:  2.315922975540161\n",
      "loss:  2.2975504398345947\n",
      "loss:  2.278607130050659\n",
      "loss:  2.3051931858062744\n",
      "loss:  2.294412136077881\n",
      "loss:  2.305569887161255\n",
      "loss:  2.3082990646362305\n",
      "loss:  2.326854944229126\n",
      "loss:  2.313267469406128\n",
      "loss:  2.3057727813720703\n",
      "loss:  2.2988245487213135\n",
      "loss:  2.3128015995025635\n",
      "loss:  2.3160269260406494\n",
      "loss:  2.291328191757202\n",
      "loss:  2.3167059421539307\n",
      "loss:  2.328477621078491\n",
      "loss:  2.2776575088500977\n",
      "loss:  2.3150546550750732\n",
      "loss:  2.295184373855591\n",
      "loss:  2.3025286197662354\n",
      "loss:  2.2891242504119873\n",
      "loss:  2.2959768772125244\n",
      "loss:  2.318796396255493\n",
      "loss:  2.324815273284912\n",
      "loss:  2.2848193645477295\n",
      "loss:  2.309236526489258\n",
      "loss:  2.3204140663146973\n",
      "loss:  2.3009650707244873\n",
      "loss:  2.3122570514678955\n",
      "loss:  2.3097691535949707\n",
      "loss:  2.3048269748687744\n",
      "loss:  2.313814878463745\n",
      "loss:  2.298508644104004\n",
      "loss:  2.3218564987182617\n",
      "loss:  2.319972515106201\n",
      "loss:  2.3013627529144287\n",
      "loss:  2.3003904819488525\n",
      "loss:  2.299968719482422\n",
      "loss:  2.2985341548919678\n",
      "loss:  2.2990787029266357\n",
      "loss:  2.3074026107788086\n",
      "loss:  2.3207733631134033\n",
      "loss:  2.293865919113159\n",
      "loss:  2.2992656230926514\n",
      "loss:  2.307129383087158\n",
      "loss:  2.29221773147583\n",
      "loss:  2.3048105239868164\n",
      "loss:  2.299373149871826\n",
      "loss:  2.298508405685425\n",
      "loss:  2.2984275817871094\n",
      "loss:  2.3061814308166504\n",
      "loss:  2.2985711097717285\n",
      "loss:  2.309354543685913\n",
      "loss:  2.2892000675201416\n",
      "loss:  2.300750732421875\n",
      "loss:  2.311985969543457\n",
      "loss:  2.310655117034912\n",
      "loss:  2.3082919120788574\n",
      "loss:  2.3205089569091797\n",
      "loss:  2.3016788959503174\n",
      "loss:  2.2991223335266113\n",
      "loss:  2.3081040382385254\n",
      "loss:  2.313302516937256\n",
      "loss:  2.3140199184417725\n",
      "loss:  2.292762279510498\n",
      "loss:  2.2914037704467773\n",
      "loss:  2.2976186275482178\n",
      "loss:  2.3194284439086914\n",
      "loss:  2.309128999710083\n",
      "loss:  2.3091585636138916\n",
      "loss:  2.301884651184082\n",
      "loss:  2.300933837890625\n",
      "loss:  2.2902469635009766\n",
      "loss:  2.29295015335083\n",
      "loss:  2.310727596282959\n",
      "loss:  2.2964863777160645\n",
      "loss:  2.313847541809082\n",
      "loss:  2.3115830421447754\n",
      "loss:  2.3190770149230957\n",
      "loss:  2.3023133277893066\n",
      "loss:  2.310654640197754\n",
      "loss:  2.2887954711914062\n",
      "loss:  2.2967987060546875\n",
      "loss:  2.3038768768310547\n",
      "loss:  2.302849054336548\n",
      "loss:  2.292940139770508\n",
      "loss:  2.3243486881256104\n",
      "loss:  2.303267240524292\n",
      "loss:  2.3101274967193604\n",
      "loss:  2.311715602874756\n",
      "loss:  2.3063502311706543\n",
      "loss:  2.3227455615997314\n",
      "loss:  2.301198720932007\n",
      "loss:  2.3090388774871826\n",
      "loss:  2.3016538619995117\n",
      "loss:  2.308441638946533\n",
      "loss:  2.3089585304260254\n",
      "loss:  2.284060001373291\n",
      "loss:  2.3195266723632812\n",
      "loss:  2.2965645790100098\n",
      "loss:  2.3194429874420166\n",
      "loss:  2.3168795108795166\n",
      "loss:  2.294123888015747\n",
      "loss:  2.300572395324707\n",
      "loss:  2.283057928085327\n",
      "loss:  2.310295581817627\n",
      "loss:  2.321058988571167\n",
      "loss:  2.319784641265869\n",
      "loss:  2.312122106552124\n",
      "loss:  2.320547580718994\n",
      "loss:  2.297983407974243\n",
      "loss:  2.3206937313079834\n",
      "loss:  2.311624050140381\n",
      "loss:  2.321107864379883\n",
      "loss:  2.310516595840454\n",
      "loss:  2.3046276569366455\n",
      "loss:  2.2930619716644287\n",
      "loss:  2.3028385639190674\n",
      "loss:  2.3034615516662598\n",
      "loss:  2.3133385181427\n",
      "loss:  2.276423215866089\n",
      "loss:  2.3124992847442627\n",
      "loss:  2.2845048904418945\n",
      "loss:  2.3011691570281982\n",
      "loss:  2.3196403980255127\n",
      "loss:  2.3025503158569336\n",
      "loss:  2.296823501586914\n",
      "loss:  2.2876076698303223\n",
      "loss:  2.2978944778442383\n",
      "loss:  2.312537431716919\n",
      "loss:  2.3027546405792236\n",
      "loss:  2.3125932216644287\n",
      "loss:  2.292179584503174\n",
      "loss:  2.312960624694824\n",
      "loss:  2.3172342777252197\n",
      "loss:  2.310518741607666\n",
      "loss:  2.287121534347534\n",
      "loss:  2.3069331645965576\n",
      "loss:  2.311113119125366\n",
      "loss:  2.30696702003479\n",
      "loss:  2.3095343112945557\n",
      "loss:  2.2969021797180176\n",
      "loss:  2.2923805713653564\n",
      "loss:  2.2956490516662598\n",
      "loss:  2.290259599685669\n",
      "loss:  2.3103671073913574\n",
      "loss:  2.3120570182800293\n",
      "loss:  2.3106746673583984\n",
      "loss:  2.3010478019714355\n",
      "loss:  2.305978298187256\n",
      "loss:  2.305096387863159\n",
      "loss:  2.319521903991699\n",
      "loss:  2.3046810626983643\n",
      "loss:  2.31728458404541\n",
      "loss:  2.298825740814209\n",
      "loss:  2.3040192127227783\n",
      "loss:  2.2829761505126953\n",
      "loss:  2.292267084121704\n",
      "loss:  2.3036818504333496\n",
      "loss:  2.313894033432007\n",
      "loss:  2.297346591949463\n",
      "loss:  2.3177008628845215\n",
      "loss:  2.289243221282959\n",
      "loss:  2.3002309799194336\n",
      "loss:  2.3032290935516357\n",
      "loss:  2.313396692276001\n",
      "loss:  2.3141350746154785\n",
      "loss:  2.3102145195007324\n",
      "loss:  2.2737746238708496\n",
      "loss:  2.293694019317627\n",
      "loss:  2.3009450435638428\n",
      "loss:  2.303006887435913\n",
      "loss:  2.326725959777832\n",
      "loss:  2.3066868782043457\n",
      "loss:  2.2992472648620605\n",
      "loss:  2.3030428886413574\n",
      "loss:  2.301696300506592\n",
      "loss:  2.289006233215332\n",
      "loss:  2.3166449069976807\n",
      "loss:  2.2985310554504395\n",
      "loss:  2.2954020500183105\n",
      "loss:  2.324101448059082\n",
      "loss:  2.311586856842041\n",
      "loss:  2.307001829147339\n",
      "loss:  2.308488130569458\n",
      "loss:  2.314140796661377\n",
      "loss:  2.3110809326171875\n",
      "loss:  2.2965381145477295\n",
      "loss:  2.293569564819336\n",
      "loss:  2.3003036975860596\n",
      "loss:  2.3018994331359863\n",
      "loss:  2.2900123596191406\n",
      "loss:  2.2735788822174072\n",
      "loss:  2.3040144443511963\n",
      "loss:  2.3035659790039062\n",
      "loss:  2.299170970916748\n",
      "loss:  2.284191846847534\n",
      "loss:  2.2958872318267822\n",
      "loss:  2.316436290740967\n",
      "loss:  2.3151698112487793\n",
      "loss:  2.313093900680542\n",
      "loss:  2.308350086212158\n",
      "loss:  2.3149800300598145\n",
      "loss:  2.3205904960632324\n",
      "loss:  2.305896759033203\n",
      "loss:  2.301750421524048\n",
      "loss:  2.3087384700775146\n",
      "loss:  2.3014423847198486\n",
      "loss:  2.3177332878112793\n",
      "loss:  2.313499927520752\n",
      "loss:  2.293724775314331\n",
      "loss:  2.31015682220459\n",
      "loss:  2.315774917602539\n",
      "loss:  2.3190433979034424\n",
      "loss:  2.2993178367614746\n",
      "loss:  2.3190114498138428\n",
      "loss:  2.3227596282958984\n",
      "loss:  2.3046505451202393\n",
      "loss:  2.315585136413574\n",
      "loss:  2.301933765411377\n",
      "loss:  2.3081295490264893\n",
      "loss:  2.2935893535614014\n",
      "loss:  2.3303418159484863\n",
      "loss:  2.3060507774353027\n",
      "loss:  2.3064663410186768\n",
      "loss:  2.311033248901367\n",
      "loss:  2.3068575859069824\n",
      "loss:  2.3088178634643555\n",
      "loss:  2.2956948280334473\n",
      "loss:  2.2928011417388916\n",
      "loss:  2.297945261001587\n",
      "loss:  2.2980246543884277\n",
      "loss:  2.3048038482666016\n",
      "loss:  2.3055925369262695\n",
      "loss:  2.284684419631958\n",
      "loss:  2.3066813945770264\n",
      "loss:  2.3136231899261475\n",
      "loss:  2.310265302658081\n",
      "loss:  2.3073511123657227\n",
      "loss:  2.3166472911834717\n",
      "loss:  2.3100521564483643\n",
      "loss:  2.3342981338500977\n",
      "loss:  2.2923738956451416\n",
      "loss:  2.2874815464019775\n",
      "loss:  2.323763847351074\n",
      "loss:  2.3198659420013428\n",
      "loss:  2.308979034423828\n",
      "loss:  2.311950206756592\n",
      "loss:  2.326403856277466\n",
      "loss:  2.308018922805786\n",
      "loss:  2.3013720512390137\n",
      "loss:  2.2950279712677\n",
      "loss:  2.31441068649292\n",
      "loss:  2.295145034790039\n",
      "loss:  2.3095126152038574\n",
      "loss:  2.315746545791626\n",
      "loss:  2.3039324283599854\n",
      "loss:  2.294322967529297\n",
      "loss:  2.291135311126709\n",
      "loss:  2.31948184967041\n",
      "loss:  2.3134946823120117\n",
      "loss:  2.308013677597046\n",
      "loss:  2.2961902618408203\n",
      "loss:  2.3074381351470947\n",
      "loss:  2.332091808319092\n",
      "loss:  2.3237287998199463\n",
      "loss:  2.303154945373535\n",
      "loss:  2.3149077892303467\n",
      "loss:  2.296468496322632\n",
      "loss:  2.2948403358459473\n",
      "loss:  2.290140151977539\n",
      "loss:  2.2992234230041504\n",
      "loss:  2.2876527309417725\n",
      "loss:  2.3013336658477783\n",
      "loss:  2.3101508617401123\n",
      "loss:  2.3001890182495117\n",
      "loss:  2.303828716278076\n",
      "loss:  2.303281307220459\n",
      "loss:  2.3161609172821045\n",
      "loss:  2.3166959285736084\n",
      "loss:  2.3174033164978027\n",
      "loss:  2.30297589302063\n",
      "loss:  2.3019282817840576\n",
      "loss:  2.295959949493408\n",
      "loss:  2.3187475204467773\n",
      "loss:  2.3162782192230225\n",
      "loss:  2.315040349960327\n",
      "loss:  2.2892534732818604\n",
      "loss:  2.311790943145752\n",
      "loss:  2.320168972015381\n",
      "loss:  2.3024985790252686\n",
      "loss:  2.3134913444519043\n",
      "loss:  2.308429002761841\n",
      "loss:  2.3054087162017822\n",
      "loss:  2.308209180831909\n",
      "loss:  2.3137919902801514\n",
      "loss:  2.30958890914917\n",
      "loss:  2.3200430870056152\n",
      "loss:  2.298544406890869\n",
      "loss:  2.307847023010254\n",
      "loss:  2.290189027786255\n",
      "loss:  2.3114094734191895\n",
      "loss:  2.3169682025909424\n",
      "loss:  2.3009181022644043\n",
      "loss:  2.3063580989837646\n",
      "loss:  2.332095146179199\n",
      "loss:  2.2853751182556152\n",
      "loss:  2.29794979095459\n",
      "loss:  2.2962889671325684\n",
      "loss:  2.2959766387939453\n",
      "loss:  2.291944980621338\n",
      "loss:  2.305803060531616\n",
      "loss:  2.328519821166992\n",
      "loss:  2.316450595855713\n",
      "loss:  2.3033735752105713\n",
      "loss:  2.3044400215148926\n",
      "loss:  2.310706377029419\n",
      "loss:  2.296776533126831\n",
      "loss:  2.2999508380889893\n",
      "loss:  2.290114641189575\n",
      "loss:  2.3121821880340576\n",
      "loss:  2.305551767349243\n",
      "loss:  2.2931318283081055\n",
      "loss:  2.3076670169830322\n",
      "loss:  2.3044872283935547\n",
      "loss:  2.3070318698883057\n",
      "loss:  2.3015384674072266\n",
      "loss:  2.310842752456665\n",
      "loss:  2.3031351566314697\n",
      "loss:  2.3061435222625732\n",
      "loss:  2.3118176460266113\n",
      "loss:  2.2945075035095215\n",
      "loss:  2.306542158126831\n",
      "loss:  2.305643081665039\n",
      "loss:  2.294698476791382\n",
      "loss:  2.319286346435547\n",
      "loss:  2.296211004257202\n",
      "loss:  2.3148915767669678\n",
      "loss:  2.3035826683044434\n",
      "loss:  2.2906339168548584\n",
      "loss:  2.30210280418396\n",
      "loss:  2.2989981174468994\n",
      "loss:  2.3010895252227783\n",
      "loss:  2.2969462871551514\n",
      "loss:  2.287292242050171\n",
      "loss:  2.312025547027588\n",
      "loss:  2.308647394180298\n",
      "loss:  2.307116746902466\n",
      "loss:  2.2924256324768066\n",
      "loss:  2.294645071029663\n",
      "loss:  2.2984707355499268\n",
      "loss:  2.2828385829925537\n",
      "loss:  2.3155524730682373\n",
      "loss:  2.293055534362793\n",
      "loss:  2.3088667392730713\n",
      "loss:  2.2945172786712646\n",
      "loss:  2.300455093383789\n",
      "loss:  2.296555757522583\n",
      "loss:  2.305663585662842\n",
      "loss:  2.3125176429748535\n",
      "loss:  2.2979495525360107\n",
      "loss:  2.297154426574707\n",
      "loss:  2.304983139038086\n",
      "loss:  2.3011043071746826\n",
      "loss:  2.300847291946411\n",
      "loss:  2.3228156566619873\n",
      "loss:  2.313669204711914\n",
      "loss:  2.3138983249664307\n",
      "loss:  2.3137013912200928\n",
      "loss:  2.306142568588257\n",
      "loss:  2.298290729522705\n",
      "loss:  2.300583600997925\n",
      "loss:  2.296574354171753\n",
      "loss:  2.3079147338867188\n",
      "loss:  2.3033790588378906\n",
      "loss:  2.2998573780059814\n",
      "loss:  2.303715467453003\n",
      "loss:  2.2997207641601562\n",
      "loss:  2.3071913719177246\n",
      "loss:  2.3056390285491943\n",
      "loss:  2.3093411922454834\n",
      "loss:  2.308880090713501\n",
      "loss:  2.321774482727051\n",
      "loss:  2.3035337924957275\n",
      "loss:  2.2984042167663574\n",
      "loss:  2.3007750511169434\n",
      "loss:  2.300523042678833\n",
      "loss:  2.289731025695801\n",
      "loss:  2.300325632095337\n",
      "loss:  2.2974228858947754\n",
      "loss:  2.3147640228271484\n",
      "loss:  2.2940027713775635\n",
      "loss:  2.3118045330047607\n",
      "loss:  2.2845091819763184\n",
      "loss:  2.3035924434661865\n",
      "loss:  2.301196813583374\n",
      "loss:  2.293145179748535\n",
      "loss:  2.294325113296509\n",
      "loss:  2.324904203414917\n",
      "loss:  2.3100385665893555\n",
      "loss:  2.3025357723236084\n",
      "loss:  2.313589096069336\n",
      "loss:  2.308619499206543\n",
      "loss:  2.318572998046875\n",
      "loss:  2.3029990196228027\n",
      "loss:  2.2920732498168945\n",
      "loss:  2.292375087738037\n",
      "loss:  2.2924787998199463\n",
      "loss:  2.302159309387207\n",
      "loss:  2.2960641384124756\n",
      "loss:  2.3199961185455322\n",
      "loss:  2.3114423751831055\n",
      "loss:  2.3147542476654053\n",
      "loss:  2.292893886566162\n",
      "loss:  2.3200631141662598\n",
      "loss:  2.295454978942871\n",
      "loss:  2.3061699867248535\n",
      "loss:  2.2920267581939697\n",
      "loss:  2.3029022216796875\n",
      "loss:  2.300168991088867\n",
      "loss:  2.298166275024414\n",
      "loss:  2.297724962234497\n",
      "loss:  2.298067331314087\n",
      "loss:  2.3099746704101562\n",
      "loss:  2.3112218379974365\n",
      "loss:  2.303072452545166\n",
      "loss:  2.3148653507232666\n",
      "loss:  2.318347454071045\n",
      "loss:  2.3107094764709473\n",
      "loss:  2.288888692855835\n",
      "loss:  2.302765130996704\n",
      "loss:  2.288858652114868\n",
      "loss:  2.319467306137085\n",
      "loss:  2.2994747161865234\n",
      "loss:  2.311333179473877\n",
      "loss:  2.312771797180176\n",
      "loss:  2.299427032470703\n",
      "loss:  2.2955539226531982\n",
      "loss:  2.2971479892730713\n",
      "loss:  2.30586838722229\n",
      "loss:  2.298987627029419\n",
      "loss:  2.2935900688171387\n",
      "loss:  2.3070273399353027\n",
      "loss:  2.3090600967407227\n",
      "loss:  2.2962310314178467\n",
      "loss:  2.3057594299316406\n",
      "loss:  2.2899038791656494\n",
      "loss:  2.302462577819824\n",
      "loss:  2.3009676933288574\n",
      "loss:  2.292861223220825\n",
      "loss:  2.3000857830047607\n",
      "loss:  2.2985708713531494\n",
      "loss:  2.3118886947631836\n",
      "loss:  2.3102426528930664\n",
      "loss:  2.287349224090576\n",
      "loss:  2.309600830078125\n",
      "loss:  2.293921947479248\n",
      "loss:  2.2981112003326416\n",
      "loss:  2.3059234619140625\n",
      "loss:  2.294461250305176\n",
      "loss:  2.3016834259033203\n",
      "loss:  2.3082497119903564\n",
      "loss:  2.3015217781066895\n",
      "loss:  2.304394006729126\n",
      "loss:  2.29510760307312\n",
      "loss:  2.3004307746887207\n",
      "loss:  2.296875\n",
      "loss:  2.30124831199646\n",
      "loss:  2.3217623233795166\n",
      "loss:  2.2858500480651855\n",
      "loss:  2.303039789199829\n",
      "loss:  2.3024284839630127\n",
      "loss:  2.315195322036743\n",
      "loss:  2.292428731918335\n",
      "loss:  2.3000121116638184\n",
      "loss:  2.2969374656677246\n",
      "loss:  2.3061587810516357\n",
      "loss:  2.3030178546905518\n",
      "loss:  2.292818546295166\n",
      "loss:  2.306584596633911\n",
      "loss:  2.3042826652526855\n",
      "loss:  2.3024187088012695\n",
      "loss:  2.3059608936309814\n",
      "loss:  2.3038740158081055\n",
      "loss:  2.2980105876922607\n",
      "loss:  2.2918787002563477\n",
      "loss:  2.306609630584717\n",
      "loss:  2.306506633758545\n",
      "loss:  2.2890706062316895\n",
      "loss:  2.2996068000793457\n",
      "loss:  2.3038570880889893\n",
      "loss:  2.299240827560425\n",
      "loss:  2.2936131954193115\n",
      "loss:  2.306746482849121\n",
      "loss:  2.317312479019165\n",
      "loss:  2.2947275638580322\n",
      "loss:  2.302455186843872\n",
      "loss:  2.3039631843566895\n",
      "loss:  2.314674139022827\n",
      "loss:  2.301194906234741\n",
      "loss:  2.3004941940307617\n",
      "loss:  2.3071787357330322\n",
      "loss:  2.3000102043151855\n",
      "loss:  2.298443555831909\n",
      "loss:  2.2933499813079834\n",
      "loss:  2.3002161979675293\n",
      "loss:  2.305046319961548\n",
      "loss:  2.304107904434204\n",
      "loss:  2.2997169494628906\n",
      "loss:  2.2995097637176514\n",
      "loss:  2.295335054397583\n",
      "loss:  2.299684524536133\n",
      "loss:  2.287475109100342\n",
      "loss:  2.3090415000915527\n",
      "loss:  2.2868361473083496\n",
      "loss:  2.3047304153442383\n",
      "loss:  2.303013563156128\n",
      "loss:  2.3065884113311768\n",
      "loss:  2.2965478897094727\n",
      "loss:  2.296553134918213\n",
      "loss:  2.3066189289093018\n",
      "loss:  2.296787977218628\n",
      "loss:  2.2878708839416504\n",
      "loss:  2.2888731956481934\n",
      "loss:  2.2953412532806396\n",
      "loss:  2.2869515419006348\n",
      "loss:  2.2978482246398926\n",
      "loss:  2.287828207015991\n",
      "loss:  2.290381908416748\n",
      "loss:  2.316781997680664\n",
      "loss:  2.30385422706604\n",
      "loss:  2.299229860305786\n",
      "loss:  2.2916927337646484\n",
      "loss:  2.3050806522369385\n",
      "loss:  2.2915825843811035\n",
      "loss:  2.2982563972473145\n",
      "loss:  2.298271656036377\n",
      "loss:  2.297102451324463\n",
      "loss:  2.2940194606781006\n",
      "loss:  2.2922370433807373\n",
      "loss:  2.2928669452667236\n",
      "loss:  2.2961087226867676\n",
      "loss:  2.2987759113311768\n",
      "loss:  2.2884838581085205\n",
      "loss:  2.3004114627838135\n",
      "loss:  2.2931230068206787\n",
      "loss:  2.295349359512329\n",
      "loss:  2.293121814727783\n",
      "loss:  2.295717716217041\n",
      "loss:  2.299363136291504\n",
      "loss:  2.2867159843444824\n",
      "loss:  2.279611825942993\n",
      "loss:  2.2781121730804443\n",
      "loss:  2.306931495666504\n",
      "loss:  2.289508819580078\n",
      "loss:  2.283949136734009\n",
      "loss:  2.291834592819214\n",
      "loss:  2.2967820167541504\n",
      "loss:  2.28202748298645\n",
      "loss:  2.290299415588379\n",
      "loss:  2.2886974811553955\n",
      "loss:  2.2813990116119385\n",
      "loss:  2.2757863998413086\n",
      "loss:  2.2751388549804688\n",
      "loss:  2.287186861038208\n",
      "loss:  2.2787108421325684\n",
      "loss:  2.2844176292419434\n",
      "loss:  2.286038875579834\n",
      "loss:  2.2686707973480225\n",
      "loss:  2.264596939086914\n",
      "loss:  2.256208896636963\n",
      "loss:  2.2626662254333496\n",
      "loss:  2.2606658935546875\n",
      "loss:  2.264064073562622\n",
      "loss:  2.2296271324157715\n",
      "loss:  2.2389469146728516\n",
      "loss:  2.2586073875427246\n",
      "loss:  2.238492727279663\n",
      "loss:  2.2131707668304443\n",
      "loss:  2.1869616508483887\n",
      "loss:  2.182248830795288\n",
      "loss:  2.179898262023926\n",
      "loss:  2.1799087524414062\n",
      "loss:  2.20181941986084\n",
      "loss:  2.11152982711792\n",
      "loss:  2.0938379764556885\n",
      "loss:  2.0626392364501953\n",
      "loss:  1.9969159364700317\n",
      "loss:  1.9556142091751099\n",
      "loss:  1.9623773097991943\n",
      "loss:  2.0588393211364746\n",
      "loss:  2.2125535011291504\n",
      "loss:  2.025925636291504\n",
      "loss:  1.9569482803344727\n",
      "loss:  1.811562418937683\n",
      "loss:  1.816436529159546\n",
      "loss:  1.7305971384048462\n",
      "loss:  1.8876893520355225\n",
      "loss:  1.932059407234192\n",
      "loss:  2.0740270614624023\n",
      "loss:  1.6125398874282837\n",
      "loss:  1.5355558395385742\n",
      "loss:  1.6885557174682617\n",
      "loss:  1.898162603378296\n",
      "loss:  1.5318093299865723\n",
      "loss:  1.674736499786377\n",
      "loss:  1.7757924795150757\n",
      "loss:  1.5258501768112183\n",
      "loss:  2.1717472076416016\n",
      "loss:  1.9764536619186401\n",
      "loss:  1.9070768356323242\n",
      "loss:  1.6378079652786255\n",
      "loss:  1.2129204273223877\n",
      "loss:  1.4032775163650513\n",
      "loss:  1.3234584331512451\n",
      "loss:  1.45186448097229\n",
      "loss:  2.173064947128296\n",
      "loss:  1.6412911415100098\n",
      "loss:  1.4906511306762695\n",
      "loss:  1.6674985885620117\n",
      "loss:  1.7622705698013306\n",
      "loss:  1.6566352844238281\n",
      "loss:  1.5313016176223755\n",
      "loss:  1.6930227279663086\n",
      "loss:  1.1857497692108154\n",
      "loss:  1.6841604709625244\n",
      "loss:  1.430335521697998\n",
      "loss:  1.5685057640075684\n",
      "loss:  1.2366611957550049\n",
      "loss:  1.229860544204712\n",
      "loss:  1.302450180053711\n",
      "loss:  2.054835319519043\n",
      "loss:  1.7280669212341309\n",
      "loss:  1.5244414806365967\n",
      "loss:  1.665836215019226\n",
      "loss:  1.5120604038238525\n",
      "loss:  1.5405751466751099\n",
      "loss:  1.5620065927505493\n",
      "loss:  1.3686063289642334\n",
      "loss:  1.5460059642791748\n",
      "loss:  1.438535213470459\n",
      "loss:  1.2660608291625977\n",
      "loss:  1.049049735069275\n",
      "loss:  1.0753369331359863\n",
      "loss:  0.9706680178642273\n",
      "loss:  1.252307653427124\n",
      "loss:  1.3899554014205933\n",
      "loss:  1.4697967767715454\n",
      "loss:  1.1700084209442139\n",
      "loss:  1.2640656232833862\n",
      "loss:  1.3160085678100586\n",
      "loss:  1.3873674869537354\n",
      "loss:  1.2526065111160278\n",
      "loss:  1.2466095685958862\n",
      "loss:  1.4619468450546265\n",
      "loss:  1.8796846866607666\n",
      "loss:  1.5523024797439575\n",
      "loss:  1.149008870124817\n",
      "loss:  0.8564521074295044\n",
      "loss:  1.6068098545074463\n",
      "loss:  1.6214261054992676\n",
      "loss:  1.7056913375854492\n",
      "loss:  1.7151117324829102\n",
      "loss:  1.4456672668457031\n",
      "loss:  1.281356692314148\n",
      "loss:  1.4267899990081787\n",
      "loss:  1.4742205142974854\n",
      "loss:  1.3359886407852173\n",
      "loss:  1.5285640954971313\n",
      "loss:  1.3162375688552856\n",
      "loss:  1.6269571781158447\n",
      "loss:  1.7230056524276733\n",
      "loss:  1.3529011011123657\n",
      "loss:  1.6378531455993652\n",
      "loss:  1.2880003452301025\n",
      "loss:  1.4864223003387451\n",
      "loss:  0.8122544884681702\n",
      "loss:  0.92216956615448\n",
      "loss:  1.0945237874984741\n",
      "loss:  1.2068355083465576\n",
      "loss:  1.217343807220459\n",
      "loss:  3.342689275741577\n",
      "loss:  2.3323795795440674\n",
      "loss:  2.284614086151123\n",
      "loss:  2.2679667472839355\n",
      "loss:  2.3431880474090576\n",
      "loss:  2.213231325149536\n",
      "loss:  2.153569459915161\n",
      "loss:  2.084721565246582\n",
      "loss:  2.0354859828948975\n",
      "loss:  1.8252450227737427\n",
      "loss:  1.4719394445419312\n",
      "loss:  1.975318193435669\n",
      "loss:  2.3434081077575684\n",
      "loss:  2.1146059036254883\n",
      "loss:  1.8661150932312012\n",
      "loss:  1.5957261323928833\n",
      "loss:  1.8532570600509644\n",
      "loss:  1.7528791427612305\n",
      "loss:  1.6825088262557983\n",
      "loss:  1.3596118688583374\n",
      "loss:  1.2161998748779297\n",
      "loss:  1.5608147382736206\n",
      "loss:  1.5937237739562988\n",
      "loss:  1.1180680990219116\n",
      "loss:  1.303645133972168\n",
      "loss:  1.8119885921478271\n",
      "loss:  1.1492900848388672\n",
      "loss:  1.0267188549041748\n",
      "loss:  1.76437246799469\n",
      "loss:  2.181297779083252\n",
      "loss:  2.087916374206543\n",
      "loss:  1.6821941137313843\n",
      "loss:  1.337031602859497\n",
      "loss:  1.3067470788955688\n",
      "loss:  5.330302715301514\n",
      "loss:  4.1970696449279785\n",
      "loss:  194.1883544921875\n",
      "loss:  577.7059936523438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20da99ad788>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimal Learning rate\n",
    "\n",
    "import math\n",
    "def find_lr(net, loss_func, init_value = 1e-8, final_value=10., beta = 0.98, bs = 32):\n",
    "    num = (train_n-1)//bs + 1 # num of batches \n",
    "    mult = (final_value/init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0.\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        batch_num += 1\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(xb)\n",
    "        loss = loss_func(outputs, yb)\n",
    "        \n",
    "        #Compute the smoothed loss\n",
    "        print(\"loss: \", loss.item())\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        \n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        \n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "            \n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "    return log_lrs, losses\n",
    "\n",
    "model_lrfinder = FashionMnistNet()\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "log_lrs, losses = find_lr(model_lrfinder, loss_func)\n",
    "\n",
    "plt.plot([10**x for x in log_lrs], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  81.08999633789062\n",
      "Accuracy:  86.37999725341797\n",
      "Accuracy:  86.88999938964844\n",
      "Accuracy:  87.94999694824219\n",
      "Accuracy:  88.05999755859375\n",
      "Accuracy:  88.52999877929688\n",
      "Accuracy:  88.58999633789062\n",
      "Accuracy:  88.01000213623047\n",
      "Accuracy:  88.9000015258789\n",
      "Accuracy:  88.58999633789062\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_wnd = FashionMnistNet()\n",
    "lr = 0.5 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 100\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model_wnd.parameters(), lr=lr)\n",
    "accuracy_vals_wnd = []\n",
    "for epoch in range(epochs):\n",
    "    model_wnd.train()\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model_wnd.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model_wnd.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        validation_size = int(test_n/10)\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model_wnd.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals_wnd.append((accuracy*100/test_n).item())\n",
    "        \n",
    "    \n",
    "        \n",
    "axis = (1 , 2 , 3 , 4, 5, 6, 7, 8, 9, 10)\n",
    "plt.plot(axis,accuracy_vals_wnd , 'r--')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.legend(('Test Accuracy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  45.00833511352539\n",
      "Accuracy:  83.17833709716797\n",
      "Accuracy:  84.83833312988281\n",
      "Accuracy:  86.36833190917969\n",
      "Accuracy:  86.5816650390625\n",
      "Accuracy:  87.07666778564453\n",
      "Accuracy:  87.44666290283203\n",
      "Accuracy:  87.9800033569336\n",
      "Accuracy:  88.38833618164062\n",
      "Accuracy:  88.78333282470703\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dfZxUdd3/8deH5c6FRWABRREBAyVRblxNJRXFzNQSTUq7TC71im6s1PQq7NFVdlVelvZLvSwLNaXSkrz0oXWlZnjfhcrugIqiIZi4CrgsN3Ivy35+f3zPuLOwCwPM2TMz5/18PM7jzJwzc85nRnnP2e/5nu8xd0dERNKjU9IFiIhIx1Lwi4ikjIJfRCRlFPwiIimj4BcRSZnOSReQj379+vmQIUOSLkNEpKTU1dWtcPf+2y4vieAfMmQItbW1SZchIlJSzOzNtpbH2tRjZpea2Xwze9nMLouW9TWzR81sYTTvE2cNIiLSWmzBb2ajgC8ARwGjgTPMbDgwDZjl7sOBWdFzERHpIHEe8Y8EnnX3De7eBDwJnAWcCcyIXjMDmBRjDSIiso042/jnAz8ys2pgI3AaUAvs4+5LAdx9qZkNaOvNZjYVmAowePDg7dZv2bKF+vp6Nm3aFFP5e6579+4MGjSILl26JF2KiMgHYgt+d19gZj8GHgXWAS8ATbvw/unAdICamprtBhSqr6+nqqqKIUOGYGYFqrpw3J3Gxkbq6+sZOnRo0uWIiHwg1pO77n67u49z9+OBlcBCYLmZDQSI5u/uzrY3bdpEdXV1UYY+gJlRXV1d1H+RiEg6xd2rZ0A0HwycDfweeBCYEr1kCvDAHmx/T0uMVbHXJyLpFHc//v+J2vi3AJe4+yozuxaYaWYXA0uAyTHXICJSXNxh40Z47z1YuzbMP/xh2GsvePlleOqpsPxjH4OxYwu++1iD392Pa2NZIzAxzv12hMbGRiZODB9j2bJlVFRU0L9/uEDu+eefp2vXrkmWJyJx2bQJ3nqrdWivXQsnnACDBsH8+XDrrduvv+UWGDMG7r4bPv95aG5uvd25c8P6p56Cr3wlLOvRo/SCv5xVV1czb948AK6++mp69uzJlVdemXBVIrKdpqYQvJ07Q1UVbNgATz4ZAjl3OvVUOOYYWLQoBG82sLPh/YtfwHnnwfPPh5Df1n33heB/+224807o1StMVVVhnnXooXDVVS3Lq6rClB2W5vzz4ayzwrLKyli+EgW/iBQfd9i8uXUw9+oFH/pQOFL+5S9br8s2i5x/fnheU9OybuPGsM0f/AC+8x1YuRJOO237fVZXh+CvqIDVq8P+9tuvJaCHDQuvGzkSfvvb1sHdqxfsv39Y//GPw5o17X+20aPD1J7sD0GMyif4J0zYftlnPhN+uTdsaPs/9L/+a5hWrIBzzmm97oknCl+jSNo0NcGqVSFsAQ4+OMxvvTU0lzQ2hnUrV8KRR8IPfxjWDxgQ/l3muvBC+PWvwQwuvTRsu6IC9t47BG9223vtBePGtRxxZ6djjw3r99kHZs9uva5nT+gU9XUZMgSee679z9S/f/iBKWHlE/wiEp9sc0mfaGitZ56BxYtDYGfDu2/fcFQNcPrp8Pe/tz7yPeGElgOqn/4U/vGPsL3q6vDe3F5w3/hGmOeGc/aI2yw0p1RVQffurd8H0KUL/OEP7X+WLl3g6KN3+6soB+UT/Ds6Qq+s3PH6fv10hC/la+tWWLcuTNnmiAULYOHCsCzbjr1lS2h7Brj6avjTn1qOxt97DwYPhjejwR6vuQYeeig87tQpBPi4cS37HD8+NMv07Rum6mo48MCW9XPmhBOXndrpUZ6toz0D2rzgX/JUPsEvUk42bICGhtbBvG5daD+urAxH3I880nrd2rVwzz3hSPjaa+GGG8KyDRtatrtlSzjJefPN4WRlrm7dYNq0cATdtSvsu284EZkN7333bXntzTeHdvjq6nA0vm2Af/vbO/58Mbdhy44p+EXisGEDvPFGaN/Onc4+Gw44AJ5+Gq67Lixbs6YlvGfNgsMPD71CLrlk++2+9hqMGAHPPgs/+lHLicCePcN88+YwHzECJk1qWZ59jUejn1xxRWgzz66rqgpH4Nlmk50Fd7bZRUqSgr8Arr766qRLkEJraoJly7YP7mOOCScRFy6E739/+/W33gqf/GToLthWh4KDDw7Bv3EjLFkSjqQPOqglfLPd/k48EW67rXWoV1WF5haAyy8P4d3e1eFnnx2m9gwbpvBOMQW/lA730F7dOfrfdsmScDHN5s0t04ABMHx4eN3Mma3XbdoEH/lIOMm4bh1861stgb16dZh/4xswdWroy33IIdvX8ItfhPDevBn+7/9C23afPqHtvE8fGDgwvG7cuHCCMbs+O/XuHdafckqY2jNyZJjaU1Gxe9+hCAp+6SibNsGLL4Yj3ezFL1/9aujSlxvO48eHJhAITR5Ll7aE9pYt4YrH3/wmrB8xIqzL9ZWvwM9/Hvp6f+5z29fxzW+G/Tc1hfbw3r1bQnnw4JZ27P33h+nTWwd2nz4t60eNCr1a2rPPPvDZz+7+9yUSo5IOfncv6oHQ3LcbTTpd/vhH+NvfoLYWXnopBPe4cVBXF9YvWhSCvVu3MPXoEZo1sj72sfBD0a1b6LbXrVu4pD1r+vRwUjG7rlu3lp4jnTuHnivZ5bnbgBDk2/YTz9WzJ3zhC4X9PkSKhJVCONXU1Pi2N1t/4403qKqqKtqhmbPj8a9du7a8x+PfuhVefTWE+5w5obvfn/4U1n3mM/Doo+EqyiOPDPOampZ2ahGJlZnVuXvNtstL9oh/0KBB1NfX09DQkHQp7cregatsNDfD66+Hk4KdO8PPfhYugc92F+zZE444IjyvrITbbw/LivCHWSTNSjb4u3TpUt5H0sWgsREefzwcydfWhiaaNWtg3rww1sjIkXDxxS1H8yNGtD7pqL7aIkWpZINfCuydd1oCftKkcOReWwuTJ4dL3EePDiMT1tSEgasgjGZ46qnJ1i0iu0zBn0ZNTaGppqEhHLHX1oaTrBCO2AcNCsE/fnz4MTjssJaToiJS8hT85W7TptBEM3t2uNpzzhw480y46abQs2XJEjj55JbmmtGjW8YA79kzLBORsqLgLyfuIcjfeSdcYQrhIqTswFpDh4bl2eFpu3QJ7fUikioK/lI3d27oK589ol+6NAR89uKiH/wgDANw9NHhoiIRST0Ff6lwD0fus2eHNvnrrgsXL91ySxgf5qCDYOLEEPDZo30IV7qKiOQo2Qu4UuOpp0J/+dmzYfnysKyyEl5+Odwp6K23wolXjU8uItsouwu4yop7GMI321wze3YYS/2jHw03wJg/PwzodcwxYRo1qmWgsgMOSLZ2ESk5Cv4krF8fettUV4fbzx13HLz7bljXowccdVQYCgHCLezOOCO5WkWk7Cj4O8KyZS0nYGfPDqNUXnYZXH99GFTstNPCcMHHHBPueNQ55z+LhjsQkQJT8HeEY48NTTk9e4aAnzat5SYd3brBHXckW5+IpIqCP27uIdgbGuCss3QDDRFJnII/bmYtNx4RESkCnZIuoOw9+SQ8+GDSVYiIfEDBH7cbb4Qrr0y6ChGRDyj445bJhNsNiogUCQV/nBobwzALRxyRdCUiIh9Q8McpkwlzHfGLSBFR8McpO+Tx2LHJ1iEikkPBH6crrgg3J+/bN+lKREQ+oOCPU6dOYbhkEZEiouCPy5o1MHVqSzu/iEiRiDX4zexyM3vZzOab2e/NrLuZDTWz58xsoZndY2Zd46whMXPnhhukZEfdFBEpErEFv5ntD3wdqHH3UUAFcC7wY+Bn7j4cWAVcHFcNiVKPHhEpUnE39XQG9jKzzkAlsBQ4Cbg3Wj8DmBRzDcnIZGDQIN0ZS0SKTmzB7+5vA9cDSwiBvwaoA1a7e1P0snpg/7beb2ZTzazWzGobGhriKjM+umJXRIpUnE09fYAzgaHAfkAP4BNtvLTNm/66+3R3r3H3mv79+8dVZjy2bAl30NIVuyJShOIclvlk4A13bwAws/uAY4HeZtY5OuofBLwTYw3J6NIFXnstjMUvIlJk4mzjXwIcbWaVZmbAROAV4HHgnOg1U4AHYqwhWbptoogUoTjb+J8jnMTNAC9F+5oOfAv4hpm9DlQDt8dVQ2L+/d/hoouSrkJEpE2x3oHL3b8HfG+bxYuBo+Lcb+IeeST06BERKUK6crfQNm6EV15Rjx4RKVoK/kJ76SX16BGRoqbgLzRdsSsiRU7BX2i9e8Opp8LgwUlXIiLSJgV/oZ17Ljz0kLpyikjRUvAXUnNzuGpXRKSIKfgLad48qKqCv/416UpERNql4C+kTAY2b4Zhw5KuRESkXQr+QspkoFcvBb+IFDUFfyFlMjB2bLjXrohIkVJCFUpTE7zwgvrvi0jRi3WsnlTZvBm++10YPz7pSkREdkjBXyg9esBVVyVdhYjITqmpp1Beew1K8RaRIpI6OuIvlH/7t3DHrWeeSboSEZEd0hF/ITQ3w9y5OrErIiVBwV8ICxfC+vUKfhEpCQr+QqirC3MFv4iUAAV/IWQy0K0bjByZdCUiIjulk7uF8MUvwnHHQZcuSVciIrJTCv5CGD48TCIiJUBNPXtq+XK4805YsSLpSkRE8qLg31NPPQUXXghvvpl0JSIieVHw76lMBjp3hlGjkq5ERCQvCv49lcmE0O/WLelKRETyouDfE+6hD/8RRyRdiYhI3hT8e6K+HhobdeGWiJQUdefcEwccAEuXqplHREqKgn9P7btv0hWIiOwSNfXsiR//OPThFxEpIQr+PXHDDfD440lXISKySxT8u2vpUli2TCd2RaTkKPh3VyYT5gp+ESkxOw1+M/uqmfXpiGJKSl0dmMGYMUlXIiKyS/I54t8XmGNmM83sVDOzuIsqCatXhyt2q6qSrkREZJeYu+/8RSHsTwEuBGqAmcDt7r4o3vKCmpoar62t7Yhd7ZrmZuik1jIRKU5mVufuNdsuzyu1PPw6LIumJqAPcK+Z/WQHOzzYzOblTO+Z2WVm1tfMHjWzhdG8dJuRFPoiUoLyaeP/upnVAT8B/g4c5u5fBo4APt3e+9z9NXcf4+5jotduAO4HpgGz3H04MCt6XlqeeAKOPx5efz3pSkREdlk+V+72A85291YDzrt7s5mdked+JgKL3P1NMzsTmBAtnwE8AXwrz+0Uh9mz4emnoV+/pCsREdll+bRV/AVYmX1iZlVm9hEAd1+Q537OBX4fPd7H3ZdG718KDGjrDWY21cxqzay2oaEhz910kEwGhg2D3r2TrkREZJflE/y3AOtynq+PluXFzLoCnwL+uCuFuft0d69x95r+/fvvylvjl8mo/76IlKx8gt88p+uPuzeza4O7fQLIuPvy6PlyMxsIEM3f3YVtJW/VKli8WGPwi0jJyif4F0cneLtE06XA4l3Yx3m0NPMAPAhMiR5PAR7YhW0lb80aOP10OPbYpCsREdktO+3Hb2YDgJuAkwAn9MS5zN13eqRuZpXAW8Awd18TLasmXAcwGFgCTHb3le1vpYj78YuIFLH2+vHvtMkmCvhzd2en7r4BqN5mWSOhl09pev996No16SpERHbbToPfzLoDFwOHAt2zy939ohjrKl6jR8OECXBL3ue3RUSKSj5t/L8ljNfzceBJYBCwNs6iitbatfDaa7DffklXIiKy2/IJ/g+5+38A6919BnA6cFi8ZRWpF14Ad3XlFJGSlk/wb4nmq81sFLA3MCS2iopZXV2YK/hFpITl0x9/ejSQ2ncIXTF7Av8Ra1XFKpOBgQPDJCJSonYY/GbWCXjP3VcBTwHDOqSqYnXGGbpwS0RK3g6DPxqI7auEfvcyeXLSFYiI7LF82vgfNbMrzeyAaCz9vmbWN/bKis2KFbBwYbj5iohICcsn+C8CLiE09dRFU/ouo505E0aMgPr6pCsREdkj+Vy5O7QjCil6mUwYf/+AA5KuRERkj+Rz5e4FbS13998Uvpwilh2KWfeaF5ESl093ziNzHncnjLOTAdIT/Js3w/z5cMUVSVciIrLH8mnq+VruczPbmzCMQ3rMnw9btujCLREpC7tyQ5WsDcDwQhdS1A46CO67D8aPT7oSEZE9lk8b/58I4/BD6AX0YdLWr793bzjrrKSrEBEpiHyO+K/PedwEvOnu6erTePfdMGoUHH540pWIiOyxfPrxLwGec/cn3f3vQKOZDYm1qmKyZQtcdBH8Jj3nskWkvOUT/H8Eci9X3RotS4cFC0KvHo3RIyJlIp/g7+zu72efRI/Tc+/BTCbM1aNHRMpEPsHfYGafyj4xszOBFfGVVGTq6qBnTxiero5MIlK+8jm5+yXgLjO7OXpeD7R5NW9ZmjsXxoyBTvn8RoqIFL98LuBaBBxtZj0Bc/d03W/3r3+FhoakqxARKZidHsaa2TVm1tvd17n7WjPrY2Y/7IjiikJlJRx4YNJViIgUTD7tF59w99XZJ9HduE6Lr6Qi8thjMG0arE3XHzkiUt7yCf4KM+uWfWJmewHddvD68vG//ws33QR77ZV0JSIiBZPPyd3fAbPM7A7C0A0XkZaROevqYPRo6Lw7QxqJiBSnfE7u/sTMXgROBgz4gbs/EntlSWtuDj16zj8/6UpERAoqr0NZd38YeBjAzMab2c/d/ZJYK0va4sXw3nu6cEtEyk5ewW9mY4DzgM8CbwD3xVlUUXjrLejTR8EvImWn3eA3sxHAuYTAbwTuIfTjP7GDakvWiSdCY2PSVYiIFNyOjvhfBZ4GPunurwOY2eUdUlWx0P11RaQM7ag756eBZcDjZnarmU0knNwtf+5w3HFwxx1JVyIiUnDtBr+73+/unwUOAZ4ALgf2MbNbzOyUDqovGUuWwDPPhOGYRUTKzE4v4HL39e5+l7ufAQwC5gHTYq8sSXV1Ya4TuyJShnZpyEl3X+nuv3L3k+IqqChkMlBRAYcdlnQlIiIFp7GG25LJwKGHaqgGESlLsQa/mfU2s3vN7FUzW2Bmx5hZXzN71MwWRvM+cdawW4YPh099auevExEpQXEPQnMj8LC7n2NmXYFK4NvALHe/1symEc4XfCvmOnbNjTcmXYGISGxiO+I3s17A8cDtEO7VGw3vfCYwI3rZDGBSXDXsli1bQndOEZEyFWdTzzCgAbjDzOaa2W1m1gPYx92XAkTzAW292cymmlmtmdU2dOQdsK65BgYOhPff3/lrRURKUJzB3xkYB9zi7mOB9exCN1B3n+7uNe5e079//7hq3F4mA337QteuHbdPEZEOFGfw1wP17v5c9Pxewg/BcjMbCBDN342xhl1XV6f++yJS1mILfndfBrxlZgdHiyYCrwAPAlOiZVOAB+KqYZctXw5vv63gF5GyFnevnq8Bd0U9ehYDFxJ+bGaa2cXAEmByzDXkb+7cMFfwi0gZizX43X0eUNPGqolx7ne3DRoEV1wBY8cmXYmISGx0M9lco0bB9dcnXYWISKw0ZEOuefNg06akqxARiZWCP2vlytDEo6t2RaTMKfizsid2jzgi2TpERGKm4M/KZMJcJ3ZFpMwp+LMyGTjwQKiuTroSEZFYKfizMhn13xeRVFB3zqxf/lI3XhGRVFDwZ514YtIViIh0CDX1ADz7LPz5zxqHX0RSQUf8AD//OTz2WBigTUSkzOmIH3RiV0RSRcG/fj28+qqCX0RSQ8H/4ovQ3KwrdkUkNRT82St2dcQvIimh4P/Sl+CVV2D//ZOuRESkQ6hXT0UFjByZdBUiIh0m3Uf8mzbBl78Mc+YkXYmISIdJd/C/9FIYqmHJkqQrERHpMOkOfp3YFZEUUvD37g1DhiRdiYhIh1HwjxsHZklXIiLSYdIb/M3NsG6dLtwSkdRJb3fOTp1gwQLYujXpSkREOlR6j/izKiqSrkBEpEOlN/i//3244IKkqxAR6XDpbep55BHo0iXpKkREOlw6j/i3boV589R/X0RSKZ3B/+qrsHGjgl9EUimdwa8rdkUkxdIZ/JWVcMIJcMghSVciItLh0hn8n/40PPGEunKKSCqlL/jdoakp6SpERBKTvuBfuBCqquCBB5KuREQkEekL/kwm3IDlwAOTrkREJBHpDP6uXeHQQ5OuREQkEbFeuWtm/wTWAluBJnevMbO+wD3AEOCfwGfcfVWcdbRSVweHH66rdkUktTriiP9Edx/j7jXR82nALHcfDsyKnncM95Yx+EVEUiqJpp4zgRnR4xnApA7b8/vvwxVXhO6cIiIpZe4e38bN3gBWAQ78yt2nm9lqd++d85pV7t6njfdOBaYCDB48+Ig333wztjpFRMqRmdXltLZ8IO7ROce7+ztmNgB41MxezfeN7j4dmA5QU1NTmF+nRYvCPXarqwuyORGRUhRrU4+7vxPN3wXuB44ClpvZQIBo/m6cNbRyySVw0kkdtjsRkWIUW/CbWQ8zq8o+Bk4B5gMPAlOil00BOuZKquyJXd1jV0RSLs6mnn2A+80su5+73f1hM5sDzDSzi4ElwOQYa2jx9tvQ0KDgF5HUiy343X0xMLqN5Y3AxLj22666ujBXV04RSbn0XLmbyUCnTjB6u98iEZFUSc89d88/PwzTUFmZdCUiIolKT/APHx4mEZGUS0dTz6pV8Nvfwrsd13NURKRYpSP4Z8+GCy4IN1kXEUm5dAR/9ubqY8YkW4eISBFIT/CPGAG9eiVdiYhI4tIR/HV16r8vIhIp/+BvbIQlSxT8IiKR8u/O2bdvGK6ha9ekKxERKQrlH/xmsN9+SVchIlI0yr+p56ab4Pbbk65CRKRolH/w//d/w0MPJV2FiEjRKO/gX7MGXn9dJ3ZFRHKUd/DPnRvmGoNfROQD5R382St2x45Ntg4RkSJS3sG/YgUMHQoDBiRdiYhI0Sjv4L/mGli4MOkqRESKSnkHP0BFRdIViIgUlfIPfhERaUXBLyKSMgp+EZGUUfCLiKSMgl9EJGUU/CIiKaPgFxFJGQW/iEjKmLsnXcNOmVkD8GbSdeyhfsCKpIsoEvouWtP30Zq+jxZ7+l0c6O79t11YEsFfDsys1t1rkq6jGOi7aE3fR2v6PlrE9V2oqUdEJGUU/CIiKaPg7zjTky6giOi7aE3fR2v6PlrE8l2ojV9EJGV0xC8ikjIKfhGRlFHwx8jMDjCzx81sgZm9bGaXJl1TMTCzCjOba2Z/TrqWpJlZbzO718xejf4/OSbpmpJiZpdH/07mm9nvzax70jV1JDP7tZm9a2bzc5b1NbNHzWxhNO9TiH0p+OPVBFzh7iOBo4FLzOzDCddUDC4FFiRdRJG4EXjY3Q8BRpPS78XM9ge+DtS4+yigAjg32ao63J3AqdssmwbMcvfhwKzo+R5T8MfI3Ze6eyZ6vJbwj3r/ZKtKlpkNAk4Hbku6lqSZWS/geOB2AHd/391XJ1tVojoDe5lZZ6ASeCfhejqUuz8FrNxm8ZnAjOjxDGBSIfal4O8gZjYEGAs8l2wlibsB+CbQnHQhRWAY0ADcETV93WZmPZIuKgnu/jZwPbAEWAqscfe/JltVUdjH3ZdCOJAEBhRiowr+DmBmPYH/AS5z9/eSricpZnYG8K671yVdS5HoDIwDbnH3scB6CvSnfKmJ2q7PBIYC+wE9zOz8ZKsqXwr+mJlZF0Lo3+Xu9yVdT8LGA58ys38CfwBOMrPfJVtSouqBenfP/hV4L+GHII1OBt5w9wZ33wLcBxybcE3FYLmZDQSI5u8WYqMK/hiZmRHabxe4+/9Lup6kuftV7j7I3YcQTtw95u6pPapz92XAW2Z2cLRoIvBKgiUlaQlwtJlVRv9uJpLSE93beBCYEj2eAjxQiI12LsRGpF3jgc8DL5nZvGjZt939LwnWJMXla8BdZtYVWAxcmHA9iXD358zsXiBD6A03l5QN3WBmvwcmAP3MrB74HnAtMNPMLib8OE4uyL40ZIOISLqoqUdEJGUU/CIiKaPgFxFJGQW/iEjKKPhFRFJGwS9Fw8zczH6a8/xKM7u6QNu+08zOKcS2drKfydEom49vs3yImW00s3k50wUF3O8EjXYq+VI/fikmm4Gzzey/3H1F0sVkmVmFu2/N8+UXA19x98fbWLfI3ccUsDSR3aIjfikmTYSLdi7fdsW2R+xmti6aTzCzJ81sppn9w8yuNbN/MbPnzewlMzsoZzMnm9nT0evOiN5fYWbXmdkcM3vRzL6Ys93Hzexu4KU26jkv2v58M/txtOy7wEeBX5rZdfl+aDNbZ2Y/NbOMmc0ys/7R8jFm9mxU1/3ZsdjN7ENm9jczeyF6T/Yz9swZ2/+u6ApYou/klWg71+dbl5Qxd9ekqSgmYB3QC/gnsDdwJXB1tO5O4Jzc10bzCcBqYCDQDXgb+H607lLghpz3P0w42BlOGCenOzAV+E70mm5ALWGgsAmEQdOGtlHnfoSrKPsT/mp+DJgUrXuCMKb8tu8ZAmwE5uVMx0XrHPiX6PF3gZujxy8CJ0SP/zPnszwHnBU97k4YwngCsAYYFH3G2YQfob7Aa7RcrNk76f/OmpKfdMQvRcXD6KW/IdyUI19zPNz7YDOwCMgO5/sSIXCzZrp7s7svJAyPcAhwCnBBNKTGc0A14YcB4Hl3f6ON/R0JPOFhQLEm4C7CuPo7s8jdx+RMT0fLm4F7ose/Az5qZnsTQvrJaPkM4HgzqwL2d/f7Adx9k7tvyKm33t2bCT8sQ4D3gE3AbWZ2NpB9raSYgl+K0Q2EtvLcsembiP5/jZowuuas25zzuDnneTOtz2NtOz6JAwZ8LSeMh3rLOPDr26nP8v0gu2lH46jsaN+538NWoHP0w3QUYYTYSYS/eiTlFPxSdNx9JTCTEP5Z/wSOiB6fCXTZjU1PNrNOUZv4MEITyCPAl6PhszGzEXncDOU54AQz62dmFcB5wJM7ec+OdAKy5y8+Bzzj7muAVWZ2XLT888CT0V9E9WY2Kaq3m5lVtrfh6F4Qe3sYGPAyQCeXRb16pGj9FPhqzvNbgQfM7HnCvUfbOxrfkdcIAb0P8CV332RmtxGaRDLRXxIN7OT2du6+1MyuAh4nHIH/xd3zGS73oJxRWgF+7e43ET7LoWZWR2in/2y0fgrhRHElrUfu/DzwKzP7T2ALOx6xsfbUe1gAAABISURBVIrwvXWPat3uxLmkj0bnFEmYma1z955J1yHpoaYeEZGU0RG/iEjK6IhfRCRlFPwiIimj4BcRSRkFv4hIyij4RURS5v8DgycY4OrZb94AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_wnd = FashionMnistNet()\n",
    "lr = 0.5 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 100  # Batches\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model_wnd.parameters(), lr=lr)\n",
    "accuracy_vals_wnd1 = []\n",
    "for epoch in range(epochs):\n",
    "    model_wnd.train()\n",
    "    for i in range((train_n-1)//bs + 1):    #training in 1bs batches\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model_wnd.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model_wnd.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        validation_size = int(train_n/10)\n",
    "        for i in range(train_n):\n",
    "            x = x_train[i].reshape(1, 1, 28, 28)\n",
    "            y = y_train[i]\n",
    "            pred = model_wnd.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/train_n).item())\n",
    "        accuracy_vals_wnd1.append((accuracy*100/train_n).item())\n",
    "        \n",
    "    \n",
    "        \n",
    "axis = (1 , 2 , 3 , 4, 5, 6, 7, 8, 9, 10)\n",
    "plt.plot(axis,accuracy_vals_wnd1 , 'r--')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.legend(('Train Accuracy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(axis,accuracy_vals_wnd , 'r--',axis,accuracy_vals_wnd1 , 'b--')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.legend(('Test Accuracy','Training Accuracy'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
